{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1122fc80-3628-4568-965a-c09b5af9ab99",
   "metadata": {},
   "source": [
    "# Regression Modelling of Ames Housing Prices \n",
    "### A Workbook Supporting the Exploratory Analysis of the Ames Housing Dataset\n",
    "\n",
    "**Prepared by:** Rebecca Stalley-Moores  \n",
    "**Date:** 20/08/2025\n",
    "\n",
    "---\n",
    "\n",
    "This workbook contains the full exploratory data analysis (EDA), data cleaning, feature engineering, and hypothesis testing conducted on the Ames Housing Dataset. This analysis proceeds to develop and evaluate linear regression models to accurately predict residential property sale prices.It serves as a companion to the formal report, providing detailed code, visualizations, and intermediate steps used to derive the insights summarized in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59ad12-539f-44a0-91d0-7b2882452966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and Import libraries\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install statsmodels\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scipy\n",
    "\n",
    "# Core data manipulation and numerical computing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import combinations\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, PolynomialFeatures, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score, \n",
    "                           make_scorer, cross_val_score)\n",
    "\n",
    "# Statsmodels imports\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor, OLSInfluence\n",
    "\n",
    "# Statistical testing\n",
    "from scipy import stats\n",
    "from scipy.stats import f_oneway, kruskal, shapiro\n",
    "\n",
    "# Data type checking\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "# IPython display\n",
    "from IPython.display import display\n",
    "\n",
    "# For setting up visuals\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961fe573-50f7-43be-ad6b-0ac6cc0e9175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('AmesHousing.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716a3b76-98ee-4144-8474-a311733bec9d",
   "metadata": {},
   "source": [
    "### 1. Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428a673-d9a0-43e4-8a88-41f025b4f487",
   "metadata": {},
   "source": [
    "#### 1.1 Size of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fcf894-2a47-43bf-a82a-afbc471ab61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716867a-14b1-43dd-987c-ff593334665b",
   "metadata": {},
   "source": [
    "The dataset has 82 columns and 2930 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247f723-dfd9-4391-8bc7-eeee9269f375",
   "metadata": {},
   "source": [
    "#### 1.2 Description of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a085360-f391-4341-b456-9cba5cd6eed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c45462-95b7-4566-84e8-89c696249f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88760565-b8ac-4766-94fe-48ed26240abb",
   "metadata": {},
   "source": [
    "#### 1.3 identify Possible Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a331f-61df-40a4-9c46-45bfd70521ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['SalePrice']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e0082-eecf-4f44-bcad-7181812c395e",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df51dfd3-a247-4f63-ac72-e1122ad0687e",
   "metadata": {},
   "source": [
    "#### 2.1 Find and Replace Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43ddb37-8160-43a4-8be8-79ab3a0fdb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out which columns contain missing valuespor\n",
    "nulls = df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending = False)\n",
    "print(nulls)\n",
    "print(nulls.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e078ae2f-33e2-4d2e-8070-6ca6bb39607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with > 1465 (50%) missing values\n",
    "df.drop([\"Pool QC\", \"Misc Feature\", \"Alley\", \"Fence\", \"Mas Vnr Type\" ], axis=1, inplace=True)\n",
    "\n",
    "# Check number of columns is now reduced to 77\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc60fc-e036-479e-92ba-c3c4dae1ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine remaining null value columns and their dtypes\n",
    "nulls = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "dtypes = df.dtypes\n",
    "\n",
    "missing_info = pd.DataFrame({\n",
    "    'Missing Values': nulls,\n",
    "    'Data Type': dtypes[nulls.index]\n",
    "}).sort_values(by='Missing Values', ascending=False)\n",
    "\n",
    "missing_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3359fe7-7b0c-41be-b303-e5160d00a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the floats first\n",
    "nulls = df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False)\n",
    "\n",
    "# Filter for columns that are float dtype\n",
    "float_nulls = [col for col in nulls.index if df[col].dtype == 'float64']\n",
    "\n",
    "print(\"Float columns with missing values:\", float_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d2c2b9-3577-4c25-95ec-c7d11cda8d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each float column, check unique values and plot distribution\n",
    "for col in float_nulls:\n",
    "    print(f\"--- {col} ---\")\n",
    "    print(\"Unique values:\", df[col].dropna().unique())  # show first 10 unique values\n",
    "    print(\"Number of unique values:\", df[col].nunique())\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(6,3))\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45c170c-b3cb-438c-a094-600696df448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at discrete numeric counts first\n",
    "df[['Bsmt Half Bath', 'Bsmt Full Bath', 'Garage Cars']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2202b9-130a-4961-a6d4-1fea2ac4f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These could be null values simply because there is no basement or garage\n",
    "# Impute Bsmt Full Bath = 0 if no basement\n",
    "df.loc[df['Total Bsmt SF'] == 0.0, 'Bsmt Full Bath'] = 0.0\n",
    "\n",
    "# Then fill the rest with median\n",
    "df['Bsmt Full Bath'].fillna(df['Bsmt Full Bath'].median(), inplace=True)\n",
    "\n",
    "df.loc[df['Total Bsmt SF'] == 0.0, 'Bsmt Half Bath'] = 0.0\n",
    "df['Bsmt Half Bath'].fillna(df['Bsmt Half Bath'].median(), inplace=True)\n",
    "\n",
    "df.loc[df['Garage Area'] == 0.0, 'Garage Cars'] = 0.0\n",
    "df['Garage Cars'].fillna(df['Garage Cars'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0514368-628a-41f4-9085-a9413f24ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check discrete numeric count null values have been resolved\n",
    "df[['Bsmt Half Bath', 'Bsmt Full Bath', 'Garage Cars']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c41440-4991-48a7-a538-2b38bb07c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the remaining features the null value likely means that there is no garage, hence the null value for 'Garage Area'\n",
    "# We can inspect against related columns to try to confirm this\n",
    "# Check Garage Yr Blt null rows: are Garage Area and Garage Cars zero or null?\n",
    "garage_null = df[df['Garage Yr Blt'].isnull()]\n",
    "print(\"Garage Yr Blt NULL rows info:\")\n",
    "print(garage_null[['Garage Area', 'Garage Cars']].describe())\n",
    "print(\"\\nSample rows:\")\n",
    "print(garage_null[['Garage Area', 'Garage Cars']].head(), \"\\n\")\n",
    "\n",
    "# Check Mas Vnr Area null rows: is it zero or something else?\n",
    "mas_vnr_null = df[df['Mas Vnr Area'].isnull()]\n",
    "print(\"Mas Vnr Area NULL rows info:\")\n",
    "print(mas_vnr_null[['Mas Vnr Area']].describe())\n",
    "print(\"\\nSample rows:\")\n",
    "print(mas_vnr_null[['Mas Vnr Area']].head(), \"\\n\")\n",
    "\n",
    "# Check Total Bsmt SF null rows: what are the basement bath counts and basement areas?\n",
    "bsmt_null = df[df['Total Bsmt SF'].isnull()]\n",
    "print(\"Total Bsmt SF NULL rows info:\")\n",
    "print(bsmt_null[['Bsmt Full Bath', 'Bsmt Half Bath', 'BsmtFin SF 1', 'BsmtFin SF 2', 'Bsmt Unf SF']].describe())\n",
    "print(\"\\nSample rows:\")\n",
    "print(bsmt_null[['Bsmt Full Bath', 'Bsmt Half Bath', 'BsmtFin SF 1', 'BsmtFin SF 2', 'Bsmt Unf SF']].head(), \"\\n\")\n",
    "\n",
    "# Check BsmtFin SF 2 null rows: related basement features\n",
    "bsmtfin2_null = df[df['BsmtFin SF 2'].isnull()]\n",
    "print(\"BsmtFin SF 2 NULL rows info:\")\n",
    "print(bsmtfin2_null[['Total Bsmt SF', 'BsmtFin SF 1', 'Bsmt Unf SF']].describe())\n",
    "print(\"\\nSample rows:\")\n",
    "print(bsmtfin2_null[['Total Bsmt SF', 'BsmtFin SF 1', 'Bsmt Unf SF']].head(), \"\\n\")\n",
    "\n",
    "# Check Bsmt Unf SF null rows:\n",
    "bsmtunf_null = df[df['Bsmt Unf SF'].isnull()]\n",
    "print(\"Bsmt Unf SF NULL rows info:\")\n",
    "print(bsmtunf_null[['Total Bsmt SF', 'BsmtFin SF 1', 'BsmtFin SF 2']].describe())\n",
    "print(\"\\nSample rows:\")\n",
    "print(bsmtunf_null[['Total Bsmt SF', 'BsmtFin SF 1', 'BsmtFin SF 2']].head(), \"\\n\")\n",
    "\n",
    "# Check Garage Area null rows:\n",
    "garage_area_null = df[df['Garage Area'].isnull()]\n",
    "print(\"Garage Area NULL rows info:\")\n",
    "print(garage_area_null[['Garage Cars', 'Garage Yr Blt']].describe())\n",
    "print(\"\\nSample rows:\")\n",
    "print(garage_area_null[['Garage Cars', 'Garage Yr Blt']].head(), \"\\n\")\n",
    "\n",
    "# Check Lot Frontage null rows: see if there is a pattern or related features to check\n",
    "lot_frontage_null = df[df['Lot Frontage'].isnull()]\n",
    "print(\"Lot Frontage NULL rows info:\")\n",
    "print(lot_frontage_null.describe(include='all'))\n",
    "print(\"\\nSample rows:\")\n",
    "print(lot_frontage_null.head(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700a67f-eded-40ce-8e48-bb7dd291a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lot Frontage appears to have genuine missing data whereas the remainder appear to be null because the feature doesn't exist\n",
    "# 1. Columns where NULL means \"feature doesn't exist\" — fill with 0 or suitable default:\n",
    "\n",
    "# Basement-related features:\n",
    "bsmt_cols = [\n",
    "    'Total Bsmt SF', 'BsmtFin SF 1', 'BsmtFin SF 2', 'Bsmt Unf SF'\n",
    "]\n",
    "\n",
    "for col in bsmt_cols:\n",
    "    df[col].fillna(0, inplace=True)\n",
    "\n",
    "# Garage-related features:\n",
    "garage_cols = ['Garage Yr Blt', 'Garage Area']\n",
    "# Garage Year Built — replace null with 0 or a special indicator like 0 to mean 'No Garage'\n",
    "df['Garage Yr Blt'].fillna(0, inplace=True)\n",
    "df['Garage Area'].fillna(0, inplace=True)\n",
    "\n",
    "# Masonry veneer area — fill NaN with 0 (no veneer)\n",
    "df['Mas Vnr Area'].fillna(0, inplace=True)\n",
    "\n",
    "# 2. Lot Frontage — actual missing data, so use median overall here.\n",
    "median_lot_frontage = df['Lot Frontage'].median()\n",
    "df['Lot Frontage'].fillna(median_lot_frontage, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd486c-742c-4658-8b35-7a469179022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check float nulls have been removed\n",
    "nulls = df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False)\n",
    "\n",
    "# Filter for columns that are float dtype\n",
    "float_nulls = [col for col in nulls.index if df[col].dtype == 'float64']\n",
    "float_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4468759-b36b-4dcf-82b8-1f0b74a3fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets look at categorical nulls\n",
    "cat_nulls = [col for col in nulls.index if df[col].dtype == 'object']\n",
    "\n",
    "print(\"Categorical columns with missing values:\", cat_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477bc37f-c5dd-4f81-84f3-aafd2c1b4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each categorical column, check unique values and plot distribution\n",
    "for col in cat_nulls:\n",
    "    print(f\"--- {col} ---\")\n",
    "    print(\"Unique values:\", df[col].dropna().unique())  # show first 10 unique values\n",
    "    print(\"Number of unique values:\", df[col].nunique())\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(6,3))\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607556a-f811-4b36-ac1d-2ad1e0b293df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above suggest that for all features apart from Electrical, missing data implies the feature doesn't exist\n",
    "# lets check this before imputing\n",
    "none_fill_map = {\n",
    "    'Fireplace Qu': df['Fireplaces'] == 0,\n",
    "    'Garage Type': (df['Garage Area'] == 0) & (df['Garage Cars'] == 0),\n",
    "    'Garage Finish': (df['Garage Area'] == 0) & (df['Garage Cars'] == 0),\n",
    "    'Garage Qual': (df['Garage Area'] == 0) & (df['Garage Cars'] == 0),\n",
    "    'Garage Cond': (df['Garage Area'] == 0) & (df['Garage Cars'] == 0),\n",
    "    'Bsmt Exposure': df['Total Bsmt SF'] == 0,\n",
    "    'BsmtFin Type 1': df['Total Bsmt SF'] == 0,\n",
    "    'BsmtFin Type 2': df['Total Bsmt SF'] == 0,\n",
    "    'Bsmt Qual': df['Total Bsmt SF'] == 0,\n",
    "    'Bsmt Cond': df['Total Bsmt SF'] == 0,\n",
    "}\n",
    "\n",
    "# Fill 'None' where the missing is due to absence of the feature\n",
    "for col, condition in none_fill_map.items():\n",
    "    df.loc[condition & df[col].isnull(), col] = 'None'\n",
    "\n",
    "# Now double-check and fill any remaining NaNs in these columns just in case\n",
    "for col in none_fill_map.keys():\n",
    "    df[col].fillna('None', inplace=True)\n",
    "\n",
    "# Fill Electrical with mode (likely just missing info)\n",
    "df['Electrical'].fillna(df['Electrical'].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32767a8-874c-41ec-8920-afc2c0f921f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check categorical nulls have been removed\n",
    "nulls = df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False)\n",
    "nulls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ccc52c-b13e-4e9c-9822-037b77c9ce2e",
   "metadata": {},
   "source": [
    "#### 2.2 Check Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351e3f18-403e-4b82-a30f-b2900af4bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Discrete Numeric Counts to Integers\n",
    "df[\"Bsmt Half Bath\"] = df[\"Bsmt Half Bath\"].astype(int)\n",
    "df[\"Bsmt Full Bath\"] = df[\"Bsmt Full Bath\"].astype(int)\n",
    "df[\"Garage Cars\"] = df[\"Garage Cars\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a127515-307c-4de8-b556-5ba1dff8a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Bsmt Half Bath\", \"Bsmt Full Bath\",\"Garage Cars\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10ad388-87f6-49be-82a0-395c7ed9dc28",
   "metadata": {},
   "source": [
    "#### 2.3 Check for duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f134ddf-7fe3-47bf-92b3-399670bd1dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecceff5f-af70-4f34-8e95-d46db00ee11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore no duplicated rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f4768d-c0ca-4c3f-998a-5d02b52e35ac",
   "metadata": {},
   "source": [
    "#### 2.4 Check for Inconsistent Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc4418-0ad1-4b6c-aa59-fe53e3e83936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already checked above but to make sure:\n",
    "cat_feats = df.select_dtypes(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802b749-b509-4b0a-880c-cefd14f6c12c",
   "metadata": {},
   "source": [
    "#### 2.5 Check for Outliers in Numerical Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a6b1f4-b919-4a2d-8013-0fbaa447b803",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe()) \n",
    "num_feats = df.select_dtypes(include='number')\n",
    "\n",
    "batch_size = 10\n",
    "n_cols = 3\n",
    "\n",
    "num_feats = df.select_dtypes(include='number')\n",
    "n_features = len(num_feats.columns)\n",
    "\n",
    "for batch_start in range(0, n_features, batch_size):\n",
    "    batch = num_feats.columns[batch_start:batch_start + batch_size]\n",
    "    n = len(batch)\n",
    "    n_rows = (n + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, feature in enumerate(batch):\n",
    "        sns.boxplot(data=df, x=feature, ax=axes[i])\n",
    "        axes[i].set_title(f'Boxplot of {feature}')\n",
    "        axes[i].set_xlabel(feature)\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure with batch index (starting from 1)\n",
    "    batch_num = (batch_start // batch_size) + 1\n",
    "    plt.savefig(f'boxplots_batch_{batch_num}.png')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372225d-ea15-458a-8b79-986aa28cba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats = df.select_dtypes(include='number').columns\n",
    "skew_threshold = 1.0\n",
    "\n",
    "log_transform_cols = []\n",
    "\n",
    "for col in num_feats:\n",
    "    skewness = df[col].skew()\n",
    "    if skewness > skew_threshold:\n",
    "        log_transform_cols.append(col)\n",
    "        # Create new column with suffix '_log'\n",
    "        df[col + '_log'] = np.log1p(df[col])\n",
    "\n",
    "print(\"Log transformed columns saved as new features:\")\n",
    "print(log_transform_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d634a2b-e499-4de9-8c4f-c3855b483f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251df5f-39a1-4cb7-887b-556b2fe68c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ae2f2-d3af-43b6-a6e6-e98cb8dc13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())\n",
    "log_transformed = [\n",
    "    \"MS SubClass_log\",\n",
    "    \"Lot Frontage_log\",\n",
    "    \"Lot Area_log\",\n",
    "    \"Mas Vnr Area_log\",\n",
    "    \"BsmtFin SF 1_log\",\n",
    "    \"BsmtFin SF 2_log\",\n",
    "    \"Total Bsmt SF_log\",\n",
    "    \"1st Flr SF_log\",\n",
    "    \"Low Qual Fin SF_log\",\n",
    "    \"Gr Liv Area_log\",\n",
    "    \"Bsmt Half Bath_log\",\n",
    "    \"Kitchen AbvGr_log\",\n",
    "    \"Wood Deck SF_log\",\n",
    "    \"Open Porch SF_log\",\n",
    "    \"Enclosed Porch_log\",\n",
    "    \"3Ssn Porch_log\",\n",
    "    \"Screen Porch_log\",\n",
    "    \"Pool Area_log\",\n",
    "    \"Misc Val_log\",\n",
    "    \"SalePrice_log\"\n",
    "]\n",
    "\n",
    "transformed_feats = df[log_transformed]\n",
    "\n",
    "batch_size = 10\n",
    "n_cols = 3\n",
    "\n",
    "n_features = len(transformed_feats.columns)\n",
    "\n",
    "for batch_start in range(0, n_features, batch_size):\n",
    "    batch = transformed_feats.columns[batch_start:batch_start + batch_size]\n",
    "    n = len(batch)\n",
    "    n_rows = (n + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, feature in enumerate(batch):\n",
    "        sns.boxplot(data=df, x=feature, ax=axes[i])\n",
    "        axes[i].set_title(f'Boxplot of {feature}')\n",
    "        axes[i].set_xlabel(feature)\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure with batch index (starting from 1)\n",
    "    batch_num = (batch_start // batch_size) + 1\n",
    "    plt.savefig(f'transformed boxplots_batch_{batch_num}.png')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9b737b-b354-4274-b166-c5666ca60110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The log transformation has not really improved the outliers so trying Yeo-Johnson transform next as often performs better when \n",
    "# there are a lot of small values, including zeros and negatives\n",
    "\n",
    "# Set skewness threshold\n",
    "skew_threshold = 0.75\n",
    "\n",
    "# Select numerical features\n",
    "num_feats = df.select_dtypes(include='number')\n",
    "\n",
    "# Calculate skewness\n",
    "skew_vals = num_feats.skew().sort_values(ascending=False)\n",
    "\n",
    "# Identify features with high skew\n",
    "high_skew_feats = skew_vals[abs(skew_vals) > skew_threshold].index.tolist()\n",
    "\n",
    "# Apply Yeo-Johnson transform\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "for feature in high_skew_feats:\n",
    "    reshaped_data = df[[feature]].values  # 2D array for sklearn\n",
    "    try:\n",
    "        transformed = pt.fit_transform(reshaped_data)\n",
    "        new_col_name = f\"{feature}_yj\"\n",
    "        df[new_col_name] = transformed\n",
    "    except Exception as e:\n",
    "        print(f\"Could not transform {feature}: {e}\")\n",
    "\n",
    "# List of new columns created\n",
    "transformed_yj = [f\"{feature}_yj\" for feature in high_skew_feats]\n",
    "print(\"Yeo-Johnson transformed features:\", transformed_yj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65027fa-4907-4295-923c-0cc000869097",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a00d16-a05c-4250-80e6-5d9206c22e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "yj_feats = df[transformed_yj]\n",
    "batch_size = 10\n",
    "n_cols = 3\n",
    "n_features = len(yj_feats.columns)\n",
    "\n",
    "for batch_start in range(0, n_features, batch_size):\n",
    "    batch = yj_feats.columns[batch_start:batch_start + batch_size]\n",
    "    n = len(batch)\n",
    "    n_rows = (n + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, feature in enumerate(batch):\n",
    "        sns.boxplot(data=df, x=feature, ax=axes[i])\n",
    "        axes[i].set_title(f'Boxplot of {feature}')\n",
    "        axes[i].set_xlabel(feature)\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure with batch index (starting from 1)\n",
    "    batch_num = (batch_start // batch_size) + 1\n",
    "    plt.savefig(f'transformed boxplots_batch_{batch_num}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b1553-959e-4c5b-90af-0162f5db52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[-35:]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd3c44c-2779-4636-b555-e4dac56a8ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This hasn't helped and it has made the data less interpretable as a whole.  \n",
    "# Capping will be used to reduce skew and influence of rare cases.\n",
    "\n",
    "# Define features to cap with their capping percentiles\n",
    "capping_config = {\n",
    "    'Lot Area': 0.99,\n",
    "    'Mas Vnr Area': 0.99,\n",
    "    'Wood Deck SF': 0.99,\n",
    "    'Open Porch SF': 0.99,\n",
    "    'Enclosed Porch': 0.99,\n",
    "    '3Ssn Porch': 0.95,\n",
    "    'Screen Porch': 0.95,\n",
    "    'Pool Area': 0.95,\n",
    "    'Misc Val': 0.99,\n",
    "    'Gr Liv Area': 0.99,\n",
    "    'SalePrice': 0.99  \n",
    "}\n",
    "\n",
    "# Create capped versions of features\n",
    "for feature, quantile in capping_config.items():\n",
    "    cap_value = df[feature].quantile(quantile)\n",
    "    new_col = f\"{feature}_capped\"\n",
    "    df[new_col] = np.where(df[feature] > cap_value, cap_value, df[feature])\n",
    "    print(f\"Capped '{feature}' at {quantile*100:.0f}th percentile: {cap_value:.2f} → New column: '{new_col}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a616b-f0a2-441c-825e-e86e0c565c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "capped_columns = [\n",
    "    \"Lot Area_capped\",\n",
    "    \"Mas Vnr Area_capped\",\n",
    "    \"Wood Deck SF_capped\",\n",
    "    \"Open Porch SF_capped\",\n",
    "    \"Enclosed Porch_capped\",\n",
    "    \"3Ssn Porch_capped\",\n",
    "    \"Screen Porch_capped\",\n",
    "    \"Pool Area_capped\",\n",
    "    \"Misc Val_capped\",\n",
    "    \"Gr Liv Area_capped\",\n",
    "    \"SalePrice_capped\"\n",
    "]\n",
    "capped_feats = df[capped_columns]\n",
    "batch_size = 10\n",
    "n_cols = 3\n",
    "n_features = len(capped_feats.columns)\n",
    "\n",
    "for batch_start in range(0, n_features, batch_size):\n",
    "    batch = capped_feats.columns[batch_start:batch_start + batch_size]\n",
    "    n = len(batch)\n",
    "    n_rows = (n + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, feature in enumerate(batch):\n",
    "        sns.boxplot(data=df, x=feature, ax=axes[i],color=\"green\")\n",
    "        axes[i].set_title(f'Boxplot of {feature}')\n",
    "        axes[i].set_xlabel(feature)\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure with batch index (starting from 1)\n",
    "    batch_num = (batch_start // batch_size) + 1\n",
    "    plt.savefig(f'transformed boxplots_batch_{batch_num}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de9d00-1075-44b6-9da3-a8287bf66afc",
   "metadata": {},
   "source": [
    "#### 2.6 Logical Inconsistencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189d9af-5684-4ebb-be5d-7d4f62159ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logical Inconsistency Checks\n",
    "\n",
    "# 1. Basement area is 0, but other basement features exist\n",
    "basement_inconsistencies = df[\n",
    "    (df[\"Total Bsmt SF\"] == 0) & (\n",
    "        (df[\"BsmtFin SF 1\"] > 0) | \n",
    "        (df[\"BsmtFin SF 2\"] > 0) | \n",
    "        (df[\"Bsmt Full Bath\"] > 0) | \n",
    "        (df[\"Bsmt Half Bath\"] > 0)\n",
    "    )\n",
    "]\n",
    "\n",
    "# 2. Garage area is 0, but garage features are filled\n",
    "garage_inconsistencies = df[\n",
    "    (df[\"Garage Area\"] == 0) & (\n",
    "        (df[\"Garage Cars\"] > 0) |\n",
    "        (df[\"Garage Yr Blt\"] > 0)\n",
    "    )\n",
    "]\n",
    "\n",
    "# 3. Remodel year before the build year\n",
    "remodel_inconsistencies = df[\n",
    "    df[\"Year Remod/Add\"] < df[\"Year Built\"]\n",
    "]\n",
    "\n",
    "# 4. 2nd floor area without 1st floor area\n",
    "second_floor_inconsistencies = df[\n",
    "    (df[\"2nd Flr SF\"] > 0) & (df[\"1st Flr SF\"] == 0)\n",
    "]\n",
    "\n",
    "# 5. Gross living area smaller than sum of 1st + 2nd + low quality finished SF\n",
    "living_area_inconsistencies = df[\n",
    "    df[\"Gr Liv Area\"] < (df[\"1st Flr SF\"] + df[\"2nd Flr SF\"] + df[\"Low Qual Fin SF\"])\n",
    "]\n",
    "\n",
    "# Display summary of findings\n",
    "print(\"Basement inconsistencies:\", len(basement_inconsistencies))\n",
    "print(\"Garage inconsistencies:\", len(garage_inconsistencies))\n",
    "print(\"Remodel year inconsistencies:\", len(remodel_inconsistencies))\n",
    "print(\"2nd floor inconsistencies:\", len(second_floor_inconsistencies))\n",
    "print(\"Living area inconsistencies:\", len(living_area_inconsistencies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa92594-7538-4737-80fa-77b610bbcab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[df['Order'] == 2237, ['Garage Cars', 'Garage Yr Blt']])\n",
    "garage_inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b55db-212c-458d-8ff6-1e0a0d6bcfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The garage features are contradictory here so will delete this row - it is the only garage inconsisteny and this is not a very small dataset\n",
    "df = df[df['Order'] != 2237]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c0f34-28f7-40cb-8f0f-e6f172634852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is 1 house where Year Remod/Add is before Year Built, which is illogical\n",
    "remodel_inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f26871-ec59-4453-abad-8979e56818d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impossible to know if original build year or remodel year is correct so this one row will be dropped\n",
    "df = df[df['Order'] != 851]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c57e1a4-dafd-472b-8a57-f2b3453ff9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Living Area inconistency there are too many rows to remove so we will flag these rows for later consideration wen modelling\n",
    "df['living_area_inconsistency'] = (\n",
    "    df['Gr Liv Area'] < (df['1st Flr SF'] + df['2nd Flr SF'] + df['Low Qual Fin SF'])\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb24bbc2-5e0c-4ca6-b74d-046ea3aa625a",
   "metadata": {},
   "source": [
    "#### 2.7 Unusual String Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6957688-a9ec-45d0-844a-4f761bbea431",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "for col in string_cols:\n",
    "    lengths = df[col].apply(len)\n",
    "    \n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"  Min length: {lengths.min()}\")\n",
    "    print(f\"  Max length: {lengths.max()}\")\n",
    "    print(f\"  Mean length: {lengths.mean():.2f}\")\n",
    "    print(f\"  95th percentile length: {lengths.quantile(0.95)}\")\n",
    "    \n",
    "    threshold = lengths.quantile(0.95)\n",
    "    unusual = df[lengths > threshold][col]\n",
    "    \n",
    "    print(f\"  Examples of long strings (> 95th percentile): {unusual.head(5).to_list()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb5b02e-92b6-4f26-9ac7-877004f085a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No unusual string lengths found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258cec26-cbd2-4ed4-9be5-7fc42240eeb5",
   "metadata": {},
   "source": [
    "#### 2.8 Blank strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43da4d-8fbe-41b4-a997-00c877c18639",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = df.select_dtypes(include='object').columns\n",
    "found_blanks = False\n",
    "\n",
    "for col in string_cols:\n",
    "    blank_count = df[col].apply(lambda x: isinstance(x, str) and x.strip() == '').sum()\n",
    "    if blank_count > 0:\n",
    "        print(f\"Column '{col}' has {blank_count} blank/whitespace-only strings.\")\n",
    "        found_blanks = True\n",
    "\n",
    "if not found_blanks:\n",
    "    print(\"No blank or whitespace-only strings found in any string column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1940d0-64ce-45a6-8400-f2fb13bb2cad",
   "metadata": {},
   "source": [
    "#### 2.9 Create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5089883b-708e-4a99-a230-10f1533e63fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age - related features\n",
    "df['house_age'] = df['Yr Sold'] - df['Year Built']\n",
    "df['remodel_age'] = df['Yr Sold'] - df['Year Remod/Add']\n",
    "df['years_to_remodel'] = df['Year Remod/Add'] - df['Year Built']\n",
    "\n",
    "# Combine all full and half bathrooms (both basement and above grade) into one feature\n",
    "df['total_bathrooms'] = (df['Bsmt Full Bath'] + 0.5 * df['Bsmt Half Bath'] + \n",
    "                         df['Full Bath'] + 0.5 * df['Half Bath'])\n",
    "\n",
    "# Sum all porch areas (Wood Deck, Open Porch, Enclosed Porch, 3Ssn Porch, Screen Porch) to get total outdoor living space\n",
    "df['total_porch_area'] = (df['Wood Deck SF'] + df['Open Porch SF'] + df['Enclosed Porch'] + \n",
    "                          df['3Ssn Porch'] + df['Screen Porch'])\n",
    "\n",
    "# Sum of finished basement areas (type 1 + type 2)\n",
    "df['total_bsmt_finished'] = df['BsmtFin SF 1'] + df['BsmtFin SF 2']\n",
    "\n",
    "# Ratio of above ground living area to total lot area — gives sense of density\n",
    "df['living_area_ratio'] = df['Gr Liv Area'] / df['Lot Area']\n",
    "\n",
    "# Combine Overall Qual and Overall Cond into a single \"quality score\": average or weighted sum\n",
    "df['avg_quality'] = df[['Overall Qual', 'Overall Cond']].mean(axis=1)\n",
    "\n",
    "# From Mo Sold, create seasons or quarters (Winter, Spring, Summer, Fall)\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3,4,5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6,7,8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "df['season_sold'] = df['Mo Sold'].apply(get_season)\n",
    "\n",
    "# Interaction of Overall Qual and Gr Liv Area\n",
    "df['qual_living_area_interaction'] = df['Overall Qual'] * df['Gr Liv Area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc69cc-332d-422c-a81b-8586bb788047",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9070f-284e-41f2-8010-94842f7b9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create a fresh copy of the df without the log transformed and yeo-johnson transformed columns\n",
    "df_no_log_yj = df[[col for col in df.columns if '_log' not in col and '_yj' not in col]].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283bddb9-42fe-4faa-94bf-4240a926d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_log_yj.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7273cfe-487d-4264-953c-ee15180f7c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three of our created features were based on original features rather than capped features in error.  Can fix this now:\n",
    "# ✅ Recreate using _capped columns\n",
    "\n",
    "# Recalculate total_porch_area using capped values\n",
    "df['total_porch_area'] = (\n",
    "    df['Wood Deck SF_capped'] + df['Open Porch SF_capped'] + \n",
    "    df['Enclosed Porch_capped'] + df['3Ssn Porch_capped'] + \n",
    "    df['Screen Porch_capped']\n",
    ")\n",
    "\n",
    "# Recalculate living_area_ratio using capped columns\n",
    "df['living_area_ratio'] = df['Gr Liv Area_capped'] / df['Lot Area_capped']\n",
    "\n",
    "# Recalculate qual_living_area_interaction using capped Gr Liv Area\n",
    "df['qual_living_area_interaction'] = df['Overall Qual'] * df['Gr Liv Area_capped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57adc895-2969-4a48-a9e3-9cb841ae4de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also to be fixed in df_no_log_yj\n",
    "\n",
    "# Recalculate total_porch_area using capped values\n",
    "df_no_log_yj['total_porch_area'] = (\n",
    "    df_no_log_yj['Wood Deck SF_capped'] + df_no_log_yj['Open Porch SF_capped'] + \n",
    "    df_no_log_yj['Enclosed Porch_capped'] + df_no_log_yj['3Ssn Porch_capped'] + \n",
    "    df_no_log_yj['Screen Porch_capped']\n",
    ")\n",
    "\n",
    "# Recalculate living_area_ratio using capped columns\n",
    "df_no_log_yj['living_area_ratio'] = df_no_log_yj['Gr Liv Area_capped'] / df_no_log_yj['Lot Area_capped']\n",
    "\n",
    "# Recalculate qual_living_area_interaction using capped Gr Liv Area\n",
    "df_no_log_yj['qual_living_area_interaction'] = df_no_log_yj['Overall Qual'] * df_no_log_yj['Gr Liv Area_capped']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d2838c-1cd6-4444-a1f8-8faf179b08b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove original columns were capped versions exist and rename df\n",
    "# List of capped columns suffixes (without _capped)\n",
    "capped_cols_togo = [\n",
    "    'Lot Area',\n",
    "    'Mas Vnr Area',\n",
    "    'Wood Deck SF',\n",
    "    'Open Porch SF',\n",
    "    'Enclosed Porch',\n",
    "    '3Ssn Porch',\n",
    "    'Screen Porch',\n",
    "    'Pool Area',\n",
    "    'Misc Val',\n",
    "    'Gr Liv Area',\n",
    "    'SalePrice'\n",
    "]\n",
    "\n",
    "# Create list of original columns to drop (those that have capped versions)\n",
    "cols_to_drop = capped_cols_togo\n",
    "\n",
    "# Drop these original columns from df_no_log_yj\n",
    "df_no_log_yj = df_no_log_yj.drop(columns=cols_to_drop)\n",
    "\n",
    "# Rename df_no_log_yj to df_new\n",
    "df_new = df_no_log_yj.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0acbebf-ef24-4f7e-a0c8-5731a59903cf",
   "metadata": {},
   "source": [
    "## 3. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53427ad2-367a-44b2-867f-5326d4474a07",
   "metadata": {},
   "source": [
    "#### 3.1 Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7153f07-efc0-49d2-b1bd-72dbe7f2aeb1",
   "metadata": {},
   "source": [
    "#### 3.1.1 Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a1afe2-4e0a-4bbf-a629-39fcb17cce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.select_dtypes(include='number').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d992293-b904-4009-8c32-3231a7f1942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As there are 49 numerical columns lets remove those with little variance as these are not very informative\n",
    "# Select numerical columns only from df_final\n",
    "num_cols = df_new.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Calculate variance for each numerical feature\n",
    "variances = df_new[num_cols].var()\n",
    "\n",
    "# Set a threshold for \"low variance\" (adjust as needed)\n",
    "low_variance_threshold = 0.01\n",
    "\n",
    "# Identify features with variance below the threshold\n",
    "low_variance_features = variances[variances < low_variance_threshold].index.tolist()\n",
    "\n",
    "print(\"Features with low variance:\")\n",
    "print(low_variance_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ed68d-a678-4ce1-b959-40aa2a1fd2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [col for col in num_cols if col not in [\"3Ssn Porch_capped\", \"Pool Area_capped\"]]\n",
    "len(num_cols)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf2215d-0822-48a0-976e-373b3565ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get all numeric columns\n",
    "num_cols_all = df_new.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Step 2: Define ID-like columns\n",
    "id_cols = ['Order', 'PID']\n",
    "\n",
    "# Step 3: Define ordinal columns (manually identified from domain knowledge)\n",
    "ordinal_cols = [\n",
    "    'Overall Qual', 'Overall Cond', 'Exter Qual', 'Exter Cond',\n",
    "    'Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure', 'BsmtFin Type 1',\n",
    "    'BsmtFin Type 2', 'Heating QC', 'Kitchen Qual', 'Functional',\n",
    "    'Fireplace Qu', 'Garage Finish', 'Garage Qual', 'Garage Cond',\n",
    "    'Paved Drive'\n",
    "]\n",
    "\n",
    "# Only keep those that are present in df_new\n",
    "ordinal_cols = [col for col in ordinal_cols if col in df_new.columns]\n",
    "\n",
    "# Step 4: Combine all exclusions\n",
    "cols_to_exclude = set(id_cols + ordinal_cols)\n",
    "\n",
    "# Step 5: Filter numerical columns for EDA\n",
    "num_cols_filtered = [col for col in num_cols_all if col not in cols_to_exclude]\n",
    "\n",
    "# Show results\n",
    "print(\"✅ Excluded columns:\")\n",
    "print(sorted(cols_to_exclude))\n",
    "print(f\"\\n✅ Final numerical columns for univariate analysis ({len(num_cols_filtered)}):\")\n",
    "print(sorted(num_cols_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa3b29-d70a-4271-97a5-f65dbe8ac3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"eda_univariate_plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "cols = num_cols_filtered  # or whatever your final list is\n",
    "n_cols = 3  # plots per row\n",
    "n_rows = (len(cols) + n_cols - 1) // n_cols  # ceil division\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each histogram with KDE and save it\n",
    "for i, col in enumerate(cols):\n",
    "    ax = axes[i]\n",
    "    sns.histplot(data=df_new, x=col, kde=True, ax=ax, bins=30, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(f'{col}', fontsize=10)\n",
    "\n",
    "    # Save each plot as its own figure too\n",
    "    single_fig = plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(data=df_new, x=col, kde=True, bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'{col} Distribution')\n",
    "    plt.tight_layout()\n",
    "    single_fig.savefig(f\"{output_dir}/{col.replace('/', '_')}_hist_kde.png\")\n",
    "    plt.close(single_fig)  # Close to avoid clutter\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b2f277-1521-49a5-9622-22578e4704b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation with the target\n",
    "correlations = df_new[cols].corr()['SalePrice_capped'].drop('SalePrice_capped')\n",
    "\n",
    "# Sort by absolute correlation value (descending)\n",
    "sorted_corr = correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "# Display top correlations (you can adjust the number)\n",
    "sorted_corr_top = correlations.loc[sorted_corr.index]\n",
    "print(\"Correlations with SalePrice_capped:\\n\")\n",
    "display(sorted_corr_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71599ec-2e98-441a-b335-edbeb3ac565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"eda_boxplots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Number of plots per row\n",
    "n_cols = 3\n",
    "n_rows = (len(cols) + n_cols - 1) // n_cols  # ceil division\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Create horizontal boxplot in the grid\n",
    "    sns.boxplot(data=df_new, x=col, ax=ax, color='lightblue')\n",
    "    ax.set_title(f'{col}', fontsize=10)\n",
    "\n",
    "    # Save individual horizontal boxplot\n",
    "    single_fig = plt.figure(figsize=(6, 4))  # Wider than tall for horizontal\n",
    "    sns.boxplot(data=df_new, x=col, color='lightblue')\n",
    "    plt.title(f'{col} Boxplot')\n",
    "    plt.tight_layout()\n",
    "    single_fig.savefig(f\"{output_dir}/{col.replace('/', '_')}_boxplot.png\")\n",
    "    plt.close(single_fig)\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e166de-5112-40ee-a762-7731c644891d",
   "metadata": {},
   "source": [
    "#### 3.1.2 Categorical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59523c21-6564-451d-afa9-3ff5fcfee3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we capture all categorical features\n",
    "# 1. Identify all object and category dtype columns\n",
    "df_new['MS SubClass'] = df_new['MS SubClass'].astype(str)\n",
    "cat_cols = df_new.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# 2. Manually check for any int/float columns that are actually categorical (e.g. encoded labels, discrete small-range integers)\n",
    "# For example, identify numeric columns with few unique values\n",
    "potential_cats = [col for col in df_new.select_dtypes(include=['int64', 'float64']).columns\n",
    "                  if df_new[col].nunique() < 10]  # You can adjust this threshold\n",
    "\n",
    "# 3. Combine with known categorical types\n",
    "cat_cols_final = cat_cols + potential_cats\n",
    "cat_cols_final\n",
    "# 4. Optionally remove any that are clearly numeric (e.g. total bathrooms)\n",
    "# You may want to filter further based on domain knowledge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb5e8b6-aebc-4597-abb4-015e2e9d8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually remove known numerical features\n",
    "to_remove = [\n",
    "    'Bsmt Full Bath', 'Bsmt Half Bath', 'Full Bath', 'Half Bath',\n",
    "    'Bedroom AbvGr', 'Kitchen AbvGr', 'Fireplaces', 'Garage Cars',\n",
    "    'Yr Sold', '3Ssn Porch_capped', 'Pool Area_capped',\n",
    "    'living_area_inconsistency'\n",
    "]\n",
    "\n",
    "# Remove them from your cat_cols_final list\n",
    "cat_cols_final = [col for col in cat_cols_final if col not in to_remove]\n",
    "cat_cols_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af94b2-6ac9-4062-9d3b-eb0888bd6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up output folder\n",
    "output_dir = \"eda_catplots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Layout for subplots\n",
    "n_cols = 3\n",
    "n_rows = (len(cat_cols_final) + n_cols - 1) // n_cols\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(cat_cols_final):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Create countplot in the subplot grid\n",
    "    sns.countplot(data=df_new, x=col, ax=ax, order=df_new[col].value_counts().index, palette='pastel')\n",
    "    ax.set_title(col)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Save individual figure\n",
    "    single_fig = plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(data=df_new, x=col, order=df_new[col].value_counts().index, palette='pastel')\n",
    "    plt.title(f'{col} Count Plot')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    single_fig.savefig(f\"{output_dir}/{col.replace('/', '_')}_countplot.png\")\n",
    "    plt.close(single_fig)\n",
    "\n",
    "# Delete any unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24102d47-c991-43fe-aaad-02951755bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cat_cols_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff8f20-465a-424e-8eae-22d41b5e0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a large number of features - need to find the most relevant features to present in written report\n",
    "\n",
    "# Bin SalePrice for categorical association\n",
    "df_new['SalePrice_bin'] = pd.qcut(df_new['SalePrice_capped'], q=4, duplicates='drop')\n",
    "\n",
    "anova_results = {}\n",
    "for col in cat_cols_final:\n",
    "    groups = [df_new.loc[df_new[col] == cat, 'SalePrice_capped'] for cat in df_new[col].dropna().unique()]\n",
    "    try:\n",
    "        f_stat, p_val = f_oneway(*groups)\n",
    "        anova_results[col] = p_val\n",
    "    except:\n",
    "        anova_results[col] = np.nan  # For features with too few groups\n",
    "\n",
    "# Sort by significance (lowest p-values)\n",
    "sorted_anova = dict(sorted(anova_results.items(), key=lambda item: item[1] if item[1] is not None else 1))\n",
    "print(sorted_anova)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18b5d8-e078-450f-a881-5fbedd5bbbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep all features apart from utilities\n",
    "cat_cols_final = [col for col in cat_cols_final if col != 'Utilities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27958102-8c03-4552-88b8-159ee8d75e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features with no variability\n",
    "remove_cols = ['Street', 'Condition 2', 'Roof Matl']\n",
    "cat_cols_final = [col for col in cat_cols_final if col not in remove_cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e475f9b2-c483-46fa-ae3c-417693d53ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check further for low variability features\n",
    "threshold = 0.90\n",
    "\n",
    "low_variance_cols = []\n",
    "\n",
    "for col in cat_cols_final:\n",
    "    # Get normalized value counts (proportion of each category)\n",
    "    top_freq = df_new[col].value_counts(normalize=True).iloc[0]\n",
    "    \n",
    "    if top_freq > threshold:\n",
    "        low_variance_cols.append(col)\n",
    "\n",
    "print(\"Low variance categorical columns (remove or consider removing):\")\n",
    "print(low_variance_cols)\n",
    "\n",
    "# Remove low variance columns from your categorical features list\n",
    "cat_cols_reduced = [col for col in cat_cols if col not in low_variance_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6387085-81f3-41e6-a349-0b1db8ea9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features with little variability\n",
    "remove_cols = ['Land Slope','Heating','Central Air','Functional','Electrical','Garage Cond', 'Paved Drive']\n",
    "cat_cols_final = [col for col in cat_cols_final if col not in remove_cols]\n",
    "cat_cols_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c183f2f-2cf2-43ae-a003-cb17af43c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_sorted = {\n",
    "    'Neighborhood': 0.0,\n",
    "    'Exter Qual': 0.0,\n",
    "    'Bsmt Qual': 0.0,\n",
    "    'Kitchen Qual': 0.0,\n",
    "    'Garage Finish': 1.72e-248,\n",
    "    'Fireplace Qu': 1.14e-240,\n",
    "    'Foundation': 1.89e-219,\n",
    "    'Garage Type': 1.25e-191,\n",
    "    'MS SubClass': 7.73e-180,         # Note: MS SubClass is categorical but omitted from cat_cols_final below\n",
    "    'BsmtFin Type 1': 2.60e-169,\n",
    "    'Heating QC': 9.55e-162,\n",
    "    'Bsmt Exposure': 1.34e-124,\n",
    "    'Exterior 1st': 1.73e-116,\n",
    "    'Exterior 2nd': 3.31e-113,\n",
    "    'Overall Cond': 3.58e-105,\n",
    "    'Sale Condition': 9.14e-97,\n",
    "    'Sale Type': 1.69e-93,\n",
    "    'MS Zoning': 4.68e-81,\n",
    "    'Lot Shape': 8.74e-64,\n",
    "    'Garage Qual': 2.51e-61,\n",
    "    'Garage Cond': 8.09e-57,\n",
    "    'Paved Drive': 2.01e-55,\n",
    "    'House Style': 7.66e-50,\n",
    "    'Roof Style': 1.91e-47,\n",
    "    'Bsmt Cond': 2.90e-34,\n",
    "    'Land Contour': 9.97e-29,\n",
    "    'Condition 1': 2.81e-26,\n",
    "    'Bldg Type': 7.05e-23,\n",
    "    'BsmtFin Type 2': 1.02e-18,\n",
    "    'Exter Cond': 4.71e-18,\n",
    "    'Lot Config': 2.46e-12,\n",
    "    'season_sold': 0.00147\n",
    "}\n",
    "\n",
    "cat_cols_final = [\n",
    " 'MS Zoning',\n",
    " 'Lot Shape',\n",
    " 'Land Contour',\n",
    " 'Lot Config',\n",
    " 'Neighborhood',\n",
    " 'Condition 1',\n",
    " 'Bldg Type',\n",
    " 'House Style',\n",
    " 'Roof Style',\n",
    " 'Exterior 1st',\n",
    " 'Exterior 2nd',\n",
    " 'Exter Qual',\n",
    " 'Exter Cond',\n",
    " 'Foundation',\n",
    " 'Bsmt Qual',\n",
    " 'Bsmt Cond',\n",
    " 'Bsmt Exposure',\n",
    " 'BsmtFin Type 1',\n",
    " 'BsmtFin Type 2',\n",
    " 'Heating QC',\n",
    " 'Kitchen Qual',\n",
    " 'Fireplace Qu',\n",
    " 'Garage Type',\n",
    " 'Garage Finish',\n",
    " 'Garage Qual',\n",
    " 'Sale Type',\n",
    " 'Sale Condition',\n",
    " 'season_sold',\n",
    " 'Overall Cond',\n",
    " 'MS SubClass'\n",
    "]\n",
    "\n",
    "# Filter anova_sorted to only include features in cat_cols_final\n",
    "anova_filtered = {k: v for k, v in anova_sorted.items() if k in cat_cols_final}\n",
    "\n",
    "# Sort filtered features by p-value ascending\n",
    "sorted_features = sorted(anova_filtered, key=anova_filtered.get)\n",
    "\n",
    "# Top 15 features by ANOVA p-value:\n",
    "top_15_features = sorted_features[:15]\n",
    "\n",
    "print(top_15_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d468c3-8169-443e-a3cd-88d6dfa75a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_final = top_15_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75926c1-c46a-410c-bf1f-f23a1b63570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain knowledge features to add back\n",
    "domain_features = [\n",
    "    'MS Zoning', 'House Style', 'Condition 1', 'Roof Style', 'Paved Drive, '\n",
    "]\n",
    "\n",
    "for feature in domain_features:\n",
    "    if feature not in cat_cols_final:\n",
    "        cat_cols_final.append(feature)\n",
    "cat_cols_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315425f4-9ece-4393-b43b-b2d8d247e25d",
   "metadata": {},
   "source": [
    "#### 3.2 Bivariate Analysis\n",
    "#### 3.2.1 Numerical Versus Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c00624-68d3-42f5-b0f5-361955d38409",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_filtered = [col for col in num_cols_filtered if is_numeric_dtype(df_new[col])]\n",
    "# Regplots for all numerical features\n",
    "output_dir = 'numeric_vs_target'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "plots_per_row = 3\n",
    "total_plots = len(num_cols_filtered)\n",
    "rows = (total_plots + plots_per_row - 1) // plots_per_row\n",
    "\n",
    "fig, axes = plt.subplots(rows, plots_per_row, figsize=(plots_per_row * 5, rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(num_cols_filtered):\n",
    "    ax = axes[i]\n",
    "    sns.regplot(data=df_new, x=feature, y='SalePrice_capped', ax=ax,\n",
    "            scatter_kws={'alpha':0.1,'color': 'mediumblue'}, line_kws={'color':'red'})\n",
    "    ax.set_title(f'{feature} vs SalePrice_capped', fontsize=10)\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('SalePrice_capped')\n",
    "\n",
    "    # Save individual plot\n",
    "    single_fig = plt.figure(figsize=(6, 4))\n",
    "    sns.regplot(data=df_new, x=feature, y='SalePrice_capped',\n",
    "            scatter_kws={'alpha':0.1, 'color': 'mediumblue'}, line_kws={'color':'red'})\n",
    "    plt.title(f'{feature} vs SalePrice_capped')\n",
    "    plt.tight_layout()\n",
    "    safe_feature = feature.replace('/', '_').replace('\\\\', '_').replace(':', '_')\n",
    "    single_fig.savefig(f\"{output_dir}/{safe_feature}_vs_SalePrice_capped.png\")\n",
    "    plt.close(single_fig)\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4325de54-d3c0-4b2e-9dbf-189608653d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for all numerical features\n",
    "# use select features being shown in written report and group thematically\n",
    "groups = {\n",
    "    \"Size & Area Features\": [\n",
    "        'Gr Liv Area_capped',\n",
    "        '1st Flr SF',\n",
    "        'Total Bsmt SF',\n",
    "        'Mas Vnr Area_capped',\n",
    "        'Lot Area_capped',\n",
    "        'Wood Deck SF_capped',\n",
    "        'total_bsmt_finished',\n",
    "        'qual_living_area_interaction'\n",
    "    ],\n",
    "    \"Bathrooms & Plumbing\": [\n",
    "        'Full Bath',\n",
    "        'total_bathrooms'\n",
    "    ],\n",
    "    \"Rooms & Interior Count\": [\n",
    "        'TotRms AbvGrd',\n",
    "        'Bedroom AbvGr',\n",
    "        'Kitchen AbvGr'\n",
    "    ],\n",
    "    \"Quality / Composite Scores\": [\n",
    "        'avg_quality',\n",
    "        'qual_living_area_interaction'\n",
    "    ],\n",
    "    \"Age / Time-Based Features\": [\n",
    "        'house_age',\n",
    "        'remodel_age',\n",
    "        'Yr Sold'\n",
    "    ],\n",
    "    \"Garage, Porch and External Features\": [\n",
    "        'Garage Area',\n",
    "        'Open Porch SF_capped',\n",
    "        'total_porch_area'\n",
    "    ],\n",
    "   \n",
    "    \"Structural / Other\": [\n",
    "        'Fireplaces'\n",
    "    ]\n",
    "}\n",
    "\n",
    "target = 'SalePrice_capped'\n",
    "output_dir = 'heatmaps'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "vmin = -1\n",
    "vmax = 1\n",
    "\n",
    "\n",
    "def plot_and_save_group_corr_heatmap(df, features, group_name, target=target, cmap='viridis', output_dir=output_dir):\n",
    "    cols = features + [target]\n",
    "    corr = df[cols].corr()\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        corr,\n",
    "        mask=mask,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap=cmap,\n",
    "        center=0,\n",
    "        vmin=0,          # Fix color range: start at 0\n",
    "        vmax=1,          # End at 1 for perfect correlation\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={'shrink': 0.7},\n",
    "        annot_kws={\"size\": 8}\n",
    "    )\n",
    "    plt.title(f'Correlation Heatmap: {group_name}', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    safe_group_name = group_name.replace(' ', '_').replace('/', '_').replace('\\\\', '_')\n",
    "    save_path = f\"{output_dir}/{safe_group_name}_corr_heatmap.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(f\"Saved heatmap: {save_path}\")\n",
    "\n",
    "for group_name, features in groups.items():\n",
    "    filtered_features = [f for f in features if f in df_new.columns]\n",
    "    if filtered_features:\n",
    "        print(f\"Plotting and saving group: {group_name} ({len(filtered_features)} features)\")\n",
    "        plot_and_save_group_corr_heatmap(df_new, filtered_features, group_name)\n",
    "    else:\n",
    "        print(f\"Skipping group: {group_name} — no features found in dataframe.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31336fa5-5a9c-4902-afdd-2c74614a15b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single comprehensive correlation heatmap for key features\n",
    "key_features = [\n",
    "    'SalePrice_capped',\n",
    "    'qual_living_area_interaction',\n",
    "    'Gr Liv Area_capped',\n",
    "    'Garage Area',\n",
    "    'total_bathrooms',\n",
    "    'Total Bsmt SF',\n",
    "    '1st Flr SF',\n",
    "    'avg_quality',\n",
    "    'Overall Cond',\n",
    "    'house_age',\n",
    "    'remodel_age',\n",
    "    'BsmtFin SF 1',\n",
    "    'Lot Area_capped'\n",
    "]\n",
    "\n",
    "# Filter features that exist in dataframe\n",
    "filtered_features = [f for f in key_features if f in df_new.columns]\n",
    "\n",
    "# Create correlation matrix\n",
    "corr = df_new[filtered_features].corr()\n",
    "\n",
    "# Create mask for upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'shrink': 0.8},\n",
    "    annot_kws={\"size\": 9}\n",
    ")\n",
    "\n",
    "plt.title('Correlation Matrix: Key Predictive Features', fontsize=16, pad=20)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save if needed\n",
    "# plt.savefig('key_features_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11e324-645c-4eed-994b-07c2243841ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As regplots for certain ordinal variables are not very informative we will replace these with boxplots \n",
    "# List of ordinal/discrete variables\n",
    "boxplot_features = [\n",
    "    'total_bathrooms', 'Full Bath',\n",
    "    'TotRms AbvGrd', 'Bedroom AbvGr', 'Kitchen AbvGr',\n",
    "    'Yr Sold', 'Fireplaces'\n",
    "]\n",
    "\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'boxplots'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through and create boxplots\n",
    "for feature in boxplot_features:\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.boxplot(data=df_new, x=feature, y='SalePrice_capped', ax=ax, palette='pastel')\n",
    "    ax.set_title(f'{feature} vs SalePrice_capped', fontsize=11)\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('SalePrice_capped')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save before showing\n",
    "    safe_name = feature.replace('/', '_').replace('\\\\', '_').replace(':', '_').replace(' ', '_')\n",
    "    fig.savefig(f'{output_dir}/{safe_name}_boxplot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35bf34a-f2a8-47e5-8f90-794459c3d335",
   "metadata": {},
   "source": [
    "#### 3.2.2 Categorical Versus Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d998033-af4d-436a-9942-f3eaeb4659a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at all categorical relationships with the target\n",
    "cat_cols_original = [\n",
    " 'MS SubClass',\n",
    " 'MS Zoning',\n",
    " 'Street',\n",
    " 'Lot Shape',\n",
    " 'Land Contour',\n",
    " 'Utilities',\n",
    " 'Lot Config',\n",
    " 'Land Slope',\n",
    " 'Neighborhood',\n",
    " 'Condition 1',\n",
    " 'Condition 2',\n",
    " 'Bldg Type',\n",
    " 'House Style',\n",
    " 'Roof Style',\n",
    " 'Roof Matl',\n",
    " 'Exterior 1st',\n",
    " 'Exterior 2nd',\n",
    " 'Exter Qual',\n",
    " 'Exter Cond',\n",
    " 'Foundation',\n",
    " 'Bsmt Qual',\n",
    " 'Bsmt Cond',\n",
    " 'Bsmt Exposure',\n",
    " 'BsmtFin Type 1',\n",
    " 'BsmtFin Type 2',\n",
    " 'Heating',\n",
    " 'Heating QC',\n",
    " 'Central Air',\n",
    " 'Electrical',\n",
    " 'Kitchen Qual',\n",
    " 'Functional',\n",
    " 'Fireplace Qu',\n",
    " 'Garage Type',\n",
    " 'Garage Finish',\n",
    " 'Garage Qual',\n",
    " 'Garage Cond',\n",
    " 'Paved Drive',\n",
    " 'Sale Type',\n",
    " 'Sale Condition',\n",
    " 'season_sold',\n",
    " 'SalePrice_bin',\n",
    " 'Overall Cond'\n",
    "]\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'catfeats_v_target'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through and create boxplots\n",
    "for feature in cat_cols_original:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(\n",
    "        data=df_new,\n",
    "        x='SalePrice_capped',\n",
    "        y=feature,\n",
    "        palette='pastel',\n",
    "        orient='h'\n",
    "    )\n",
    "    plt.title(f'{feature} vs SalePrice_capped', fontsize=12)\n",
    "    plt.xlabel('SalePrice_capped')\n",
    "    plt.ylabel(feature)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save before showing\n",
    "    safe_name = feature.replace('/', '_').replace('\\\\', '_').replace(':', '_').replace(' ', '_')\n",
    "    plt.savefig(f'{output_dir}/{safe_name}_boxplot.png')\n",
    "    plt.show()\n",
    "    plt.close()  # Prevent overlap in subsequent plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d31e449-9f38-4ebf-a2da-5c8f5a30e78e",
   "metadata": {},
   "source": [
    "#### 3.3 Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5753a1a-a1cf-4e5a-922e-b9006223ef3a",
   "metadata": {},
   "source": [
    "#### 3.3.1. Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e44b6db-6ea8-4753-810a-031065b67552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets use our numerical features used in the written report\n",
    "report_num_feats = [\n",
    "    \"Gr Liv Area_capped\",\n",
    "    \"1st Flr SF\",\n",
    "    \"Total Bsmt SF\",\n",
    "    \"Mas Vnr Area_capped\",\n",
    "    \"Lot Area_capped\",\n",
    "    \"Wood Deck SF_capped\",\n",
    "    \"total_bsmt_finished\",\n",
    "    \"qual_living_area_interaction\",\n",
    "    \"Full Bath\",\n",
    "    \"total_bathrooms\",\n",
    "    \"TotRms AbvGrd\",\n",
    "    \"Bedroom AbvGr\",\n",
    "    \"Kitchen AbvGr\",\n",
    "    \"avg_quality\",\n",
    "    \"house_age\",\n",
    "    \"remodel_age\",\n",
    "    \"Yr Sold\",\n",
    "    \"Garage Area\",\n",
    "    \"Open Porch SF_capped\",\n",
    "    \"total_porch_area\",\n",
    "    \"MS SubClass\",\n",
    "    \"Fireplaces\"\n",
    "]\n",
    "\n",
    "features = report_num_feats + [\"SalePrice_capped\"]\n",
    "\n",
    "corr_matrix = df_new[features].corr()\n",
    "\n",
    "# Create mask for upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='coolwarm',\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    mask=mask,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.7},\n",
    "    annot_kws={\"size\": 8}  # Smaller font size for annotation numbers\n",
    ")\n",
    "plt.title('Correlation Matrix of Selected Features with SalePrice_capped')\n",
    "plt.tight_layout()\n",
    "plt.savefig('multivariate_analysis/selected_feature_correlation_matrix_masked.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdb3820-8e08-4023-8677-2a54edc33ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list correlations greater than .78\n",
    "# Get the absolute values of correlations\n",
    "abs_corr = corr_matrix.abs()\n",
    "\n",
    "# We'll create an empty list to store tuples of (feature1, feature2, correlation)\n",
    "corr_pairs = []\n",
    "\n",
    "# Iterate over the upper triangle (to avoid duplicates and self-correlation)\n",
    "for i in range(len(abs_corr.columns)):\n",
    "    for j in range(i+1, len(abs_corr.columns)):\n",
    "        corr_value = abs_corr.iloc[i, j]\n",
    "        if corr_value >= 0.78:\n",
    "            feature1 = abs_corr.columns[i]\n",
    "            feature2 = abs_corr.columns[j]\n",
    "            # Use the original correlation value with sign\n",
    "            original_corr = corr_matrix.loc[feature1, feature2]\n",
    "            corr_pairs.append((feature1, feature2, original_corr))\n",
    "\n",
    "# Sort by absolute correlation descending\n",
    "corr_pairs_sorted = sorted(corr_pairs, key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "# Print nicely\n",
    "for f1, f2, val in corr_pairs_sorted:\n",
    "    print(f\"{f1} and {f2}: {val:.3f}\")\n",
    "\n",
    "# If you want the list for further use:\n",
    "# corr_pairs_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b29cb-2473-4a10-96ff-8375cf9bc400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list correlations between .6 and .77\n",
    "# Get the absolute values of correlations\n",
    "abs_corr = corr_matrix.abs()\n",
    "\n",
    "# We'll create an empty list to store tuples of (feature1, feature2, correlation)\n",
    "corr_pairs = []\n",
    "\n",
    "# Iterate over the upper triangle (to avoid duplicates and self-correlation)\n",
    "for i in range(len(abs_corr.columns)):\n",
    "    for j in range(i+1, len(abs_corr.columns)):\n",
    "        corr_value = abs_corr.iloc[i, j]\n",
    "        if 0.6 <= corr_value < 0.78:\n",
    "            feature1 = abs_corr.columns[i]\n",
    "            feature2 = abs_corr.columns[j]\n",
    "            # Use the original correlation value with sign\n",
    "            original_corr = corr_matrix.loc[feature1, feature2]\n",
    "            corr_pairs.append((feature1, feature2, original_corr))\n",
    "\n",
    "# Sort by absolute correlation descending\n",
    "corr_pairs_sorted = sorted(corr_pairs, key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "# Print nicely\n",
    "for f1, f2, val in corr_pairs_sorted:\n",
    "    print(f\"{f1} and {f2}: {val:.3f}\")\n",
    "\n",
    "# If you want the list for further use:\n",
    "# corr_pairs_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e89b5-8ffc-43d8-9131-84e1142d9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustermap\n",
    "# Clean original correlation matrix by dropping all-NaN rows and columns\n",
    "corr_matrix_clean = corr_matrix.dropna(axis=0, how='all').dropna(axis=1, how='all')\n",
    "\n",
    "# Plot clustered heatmap of the cleaned top 15 correlation matrix\n",
    "sns.clustermap(corr_matrix_clean, annot=True, fmt=\".2f\", cmap='coolwarm', figsize=(12, 12), annot_kws={\"size\":8})\n",
    "plt.title('Clustered Correlation Matrix for Selected Features and SalePrice_capped')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d453c-e4d1-41bb-bfe4-dc74cc32ca80",
   "metadata": {},
   "source": [
    "#### 3.3.2 Grouped Boxplots (categorical vs numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8efc7dd-bdd0-48b0-a838-760b74975fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical and numerical features\n",
    "cat_feats = ['Neighborhood', 'Overall Qual', 'Garage Type', 'House Style', 'MS Zoning']\n",
    "num_feats = [\n",
    "    'qual_living_area_interaction', \n",
    "    'Garage Area', \n",
    "    'Total Bsmt SF',\n",
    "    '1st Flr SF',\n",
    "    'total_bathrooms'\n",
    "]\n",
    "\n",
    "# Folder to save plots\n",
    "save_dir = 'multivariate/grouped_boxplots_sorted'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Loop through all combinations\n",
    "for cat in cat_feats:\n",
    "    for num in num_feats:\n",
    "        plt.figure(figsize=(14, 6))\n",
    "\n",
    "        # Sort categories by median of numerical feature\n",
    "        order = df_new.groupby(cat)[num].median().sort_values().index\n",
    "\n",
    "        # Plot\n",
    "        sns.boxplot(data=df_new, x=cat, y=num, order=order, palette='Blues')\n",
    "\n",
    "        plt.title(f'{num} by {cat} (sorted by median {num})')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show inline\n",
    "       \n",
    "\n",
    "        # Save figure\n",
    "        filename = f\"{save_dir}/{num}_by_{cat}_sorted.png\"\n",
    "        plt.savefig(filename)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a321405-fa13-4fbc-9689-03fe6705c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.groupby(\"Garage Type\")[\"total_bathrooms\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027108da-04d2-4c87-a49e-53823dba1010",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.groupby(\"House Style\")[\"total_bathrooms\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c7c65d-97b3-41bc-b4ae-aaf05ad16429",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.groupby(\"MS Zoning\")[\"total_bathrooms\"].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f8883-daa9-4fa7-b21f-b61d4f98e647",
   "metadata": {},
   "source": [
    "## 4. Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d16a6ba-1eaa-48d7-a3f5-9da5f283398c",
   "metadata": {},
   "source": [
    "Group qual_living_area_interaction into categories and compare sale prices.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af07e96-78cc-49be-8dd8-682161a6c0be",
   "metadata": {},
   "source": [
    "Null hypothesis (H0): The mean sale price is the same across all groups (Low, Medium, High).\n",
    "  \n",
    "Alternative hypothesis (HA): At least one group has a different mean sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e26b5-b129-415b-97e6-009af55a6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create groups\n",
    "df_new['qual_group'] = pd.qcut(df_new['qual_living_area_interaction'], q=3, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Step 2: Visualize\n",
    "sns.boxplot(x='qual_group', y='SalePrice_capped', data=df_new)\n",
    "plt.title('SalePrice_capped distribution by qual_living_area_interaction groups')\n",
    "\n",
    "\n",
    "# Step 3: Extract sale prices per group\n",
    "low_prices = df_new.loc[df_new['qual_group'] == 'Low', 'SalePrice_capped']\n",
    "med_prices = df_new.loc[df_new['qual_group'] == 'Medium', 'SalePrice_capped']\n",
    "high_prices = df_new.loc[df_new['qual_group'] == 'High', 'SalePrice_capped']\n",
    "\n",
    "# Step 4: ANOVA test\n",
    "f_stat, p_val = stats.f_oneway(low_prices, med_prices, high_prices)\n",
    "print(f\"ANOVA F-statistic: {f_stat}, p-value: {p_val}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_val < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a statistically significant difference in mean sale price between at least two groups of qual_living_area_interaction.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: No significant difference in sale prices across groups.\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de347fe-f1da-4ff5-997f-643ca066c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a Kruskal–Wallis test to the hypothesis due to right-skewed distributions\n",
    "kw_stat, kw_pval = kruskal(low_prices, med_prices, high_prices)\n",
    "print(f\"Kruskal-Wallis statistic: {kw_stat}, p-value: {kw_pval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1420e01-580e-47b0-a7ef-576f1a9578c0",
   "metadata": {},
   "source": [
    "### Regression Modelling Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3134abf9-17f8-4b19-a988-64d02ff0fb99",
   "metadata": {},
   "source": [
    "#### Model 1: Simple Linear Regression on core features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc2180-0829-4721-8bbd-7c5be9f67b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ef3e7-3730-4dc4-8359-f16ab9b73561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your feature list and target\n",
    "features = [\n",
    "    'qual_living_area_interaction',\n",
    "    'Gr Liv Area_capped',\n",
    "    'avg_quality',\n",
    "    'Garage Area',\n",
    "    'total_bathrooms'\n",
    "]\n",
    "\n",
    "target = 'SalePrice_capped'\n",
    "\n",
    "# DataFrame to store evaluation metrics\n",
    "results = pd.DataFrame(columns=['Feature', 'R2', 'MSE', 'RMSE', 'MAE'])\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Loop through features\n",
    "for i, feature in enumerate(features):\n",
    "    X = df_new[[feature]]\n",
    "    y = df_new[target]\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results.loc[i] = [feature, r2, mse, rmse, mae]\n",
    "\n",
    "    # Residuals\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    # Plot residuals\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_pred, y=residuals)\n",
    "    plt.axhline(0, linestyle='--', color='red')\n",
    "    plt.title(f'Residuals vs Predicted Values: {feature}')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "\n",
    "    # Save plot to file\n",
    "    filename = f'residuals_vs_predicted_{feature.replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Show the results DataFrame\n",
    "print(results.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8073c67-6d87-4a7a-b51d-8f03c17b36b6",
   "metadata": {},
   "source": [
    "##  5. Preprocessing for Modelling ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ec7c3-8554-4f84-bc6f-88a0da04e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06592094-df84-48f0-af6f-850fb92f9d43",
   "metadata": {},
   "source": [
    "#### 5.1 Encode ordinal categorical variables as integers ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ded034-350e-48e1-9783-f93b4021ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numeric and categorical first\n",
    "numeric_cols = df_new.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = df_new.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "\n",
    "# Show unique counts for each categorical col to help spot ordinal vs nominal\n",
    "for col in categorical_cols:\n",
    "    print(f\"{col}: {df_new[col].nunique()} unique values\")\n",
    "    print(df_new[col].unique()[:20], \"\\n\")  # show first 20 categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7693984e-5a20-45e6-a12f-88e3b2cc8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now manually create the ordinal list based oabove and domain knowledge\n",
    "ordinal_cols = [\n",
    "    'Overall Qual',\n",
    "    'Overall Cond',\n",
    "    'Exter Qual',\n",
    "    'Exter Cond',\n",
    "    'Bsmt Qual',\n",
    "    'Bsmt Cond',\n",
    "    'Bsmt Exposure',\n",
    "    'BsmtFin Type 1',\n",
    "    'BsmtFin Type 2',\n",
    "    'Heating QC',\n",
    "    'Kitchen Qual',\n",
    "    'Functional',\n",
    "    'Fireplace Qu',\n",
    "    'Garage Qual',\n",
    "    'Garage Cond',\n",
    "    'Paved Drive',\n",
    "    'qual_group'\n",
    "]# e.g. quality ratings\n",
    "nominal_cols = [col for col in categorical_cols if col not in ordinal_cols]\n",
    "\n",
    "print(\"Ordinal columns:\", ordinal_cols)\n",
    "print(\"Nominal columns:\", nominal_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b942f-9e13-462f-8ee1-4d9e375fcf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode ordinal features as integers\n",
    "# Define mappings for ordinal features\n",
    "ordinal_mappings = {\n",
    "    'Exter Qual': {'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n",
    "    'Exter Cond': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "    'Bsmt Qual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "    'Bsmt Cond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "    'Bsmt Exposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
    "    'BsmtFin Type 1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
    "    'BsmtFin Type 2': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
    "    'Heating QC': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "    'Kitchen Qual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "    'Functional': {'Sal': 1, 'Sev': 2, 'Maj2': 3, 'Maj1': 4, 'Min2': 5, 'Min1': 6, 'Mod': 7, 'Typ': 8},\n",
    "    'Fireplace Qu': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "    'Garage Qual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "    'Garage Cond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "    'Paved Drive': {'N': 0, 'P': 1, 'Y': 2},\n",
    "    'qual_group': {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "}\n",
    "\n",
    "# Apply mappings to the dataframe\n",
    "for feature, mapping in ordinal_mappings.items():\n",
    "    df_new[feature] = df_new[feature].map(mapping)\n",
    "\n",
    "# Also keep Overall Qual and Overall Cond as integers (already numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea77c6d-3532-4f1d-9ed8-6dc73e11427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all ordinal columns\n",
    "ordinal_cols = list(ordinal_mappings.keys())\n",
    "\n",
    "# Check unique values in each ordinal column\n",
    "for col in ordinal_cols:\n",
    "    print(f\"{col}: {df_new[col].unique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde0119-5c39-4ee4-b48e-71955ba17e2f",
   "metadata": {},
   "source": [
    "#### 5.2 One-hot encode nominal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5806644-0dad-4736-ac26-03f4100ca08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of nominal columns (exclude ordinal ones)\n",
    "nominal_cols = [col for col in df_new.select_dtypes(include='object').columns \n",
    "                if col not in ordinal_cols]\n",
    "\n",
    "# One-hot encode nominal columns\n",
    "df_new = pd.get_dummies(df_new, columns=nominal_cols, drop_first=True)\n",
    "\n",
    "# Quick check\n",
    "print(df_new.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768a0fcc-795b-4dfa-b8bf-22558d7680c0",
   "metadata": {},
   "source": [
    "#### 5.3 Create Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06381baf-aa30-495d-be22-ac3fe2c685e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which numeric features would benefit from polynomial transformation\n",
    "numeric_features = df_new.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "\n",
    "# Make sure X contains only numeric features\n",
    "X = df_new[numeric_features].drop(columns=['SalePrice_capped'])\n",
    "y = df_new['SalePrice_capped']\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for feature in X.columns:\n",
    "    original_corr = np.corrcoef(X[feature], y)[0, 1]\n",
    "    squared_corr = np.corrcoef(X[feature]**2, y)[0, 1]\n",
    "    cubic_corr = np.corrcoef(X[feature]**3, y)[0, 1]\n",
    "    \n",
    "    results.append({\n",
    "        \"Feature\": feature,\n",
    "        \"Original_corr\": original_corr,\n",
    "        \"Squared_corr\": squared_corr,\n",
    "        \"Cubic_corr\": cubic_corr,\n",
    "        \"Squared_better\": abs(squared_corr) > abs(original_corr),\n",
    "        \"Cubic_better\": abs(cubic_corr) > abs(original_corr)\n",
    "    })\n",
    "\n",
    "df_poly_check = pd.DataFrame(results)\n",
    "\n",
    "# Features that may benefit from polynomial transform\n",
    "poly_candidates = df_poly_check[(df_poly_check['Squared_better']) | (df_poly_check['Cubic_better'])]\n",
    "with pd.option_context('display.max_columns', None, 'display.width', 120):\n",
    "    print(poly_candidates[['Feature', 'Original_corr', 'Squared_corr', 'Cubic_corr', 'Squared_better', 'Cubic_better']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc6c54f-5c2a-4f28-8c69-44b6226f0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_poly = [\n",
    "    'Overall Qual', 'Overall Cond', 'Exter Qual', 'Bsmt Qual', \n",
    "    'Kitchen Qual', 'Heating QC', 'Fireplace Qu', 'Garage Qual', \n",
    "    'Paved Drive', 'BsmtFin Type 1', 'BsmtFin Type 2'\n",
    "]\n",
    "continuous_poly = [\n",
    "    'Order', 'Garage Cars', 'Garage Yr Blt', 'Mo Sold', 'Yr Sold', \n",
    "    '2nd Flr SF', 'BsmtFin SF 2', 'Bsmt Unf SF', 'Screen Porch_capped', \n",
    "    'living_area_ratio', 'avg_quality'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41263251-3eb2-413b-b832-bdfaede37f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Continuous features\n",
    "poly_cont = PolynomialFeatures(degree=3, include_bias=False, interaction_only=False)\n",
    "cont_transformed = poly_cont.fit_transform(df_new[continuous_poly])\n",
    "cont_feature_names = poly_cont.get_feature_names_out(continuous_poly)\n",
    "df_cont_poly = pd.DataFrame(cont_transformed, columns=cont_feature_names, index=df_new.index)\n",
    "\n",
    "# 2. Ordinal features\n",
    "poly_ord = PolynomialFeatures(degree=3, include_bias=False, interaction_only=False)\n",
    "ord_transformed = poly_ord.fit_transform(df_new[ordinal_poly])\n",
    "ord_feature_names = poly_ord.get_feature_names_out(ordinal_poly)\n",
    "df_ord_poly = pd.DataFrame(ord_transformed, columns=ord_feature_names, index=df_new.index)\n",
    "\n",
    "# Combine with the original dataset\n",
    "df_new = pd.concat([df_new, df_cont_poly, df_ord_poly], axis=1)\n",
    "\n",
    "# Quick checks\n",
    "print(df_new.shape)\n",
    "print(df_cont_poly.head())\n",
    "print(df_ord_poly.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3410c-e76e-4081-b7fa-6740bb9ea166",
   "metadata": {},
   "source": [
    "#### 5.4 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155fb68-5ef9-4a75-a2e7-e33bcbb13464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns before scaling\n",
    "\n",
    "print(df_new.shape)\n",
    "cols_to_scale = df_new.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(df_new[cols_to_scale].shape)\n",
    "non_numeric_cols = df_new.columns.difference(cols_to_scale)\n",
    "print(df_new[non_numeric_cols].shape)\n",
    "print(df_new.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9ef6d-0766-4b08-a0ab-b9898926490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a disparity in the column sums to be investigated\n",
    "\n",
    "cols_to_scale = list(df_new.select_dtypes(include=['int64', 'float64']).columns)\n",
    "\n",
    "extra_cols = []\n",
    "for col in cols_to_scale:\n",
    "    series = df_new[col]  # This will be a Series if col is a scalar name\n",
    "    if hasattr(series, \"dtype\"):  # Check that it's a Series\n",
    "        if str(series.dtype) not in ['int64', 'float64']:\n",
    "            extra_cols.append(col)\n",
    "    else:\n",
    "        # If we ever hit a multi-column name (weird case), flag it\n",
    "        extra_cols.append(col)\n",
    "\n",
    "print(extra_cols)\n",
    "print(df_new[extra_cols].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36acbaf6-4d99-4f5a-a6ff-374ec454b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate column names (including all occurrences, not just the 2nd+)\n",
    "dup_cols = df_new.columns[df_new.columns.duplicated()].unique()\n",
    "\n",
    "for col in dup_cols:\n",
    "    # Get all columns with this name (could be more than 2)\n",
    "    same_name_cols = [c for c in df_new.columns if c == col]\n",
    "    \n",
    "    print(f\"\\n--- Column name: '{col}' ---\")\n",
    "    display(df_new[same_name_cols].head())  # show first few rows\n",
    "    # Also check if they're all identical\n",
    "    identical = df_new[same_name_cols].nunique(axis=1).eq(1).all()\n",
    "    print(f\"Identical across duplicates? {identical}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab1151-87ff-4da6-b3e2-27a83ccd2f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df_new = df_new.loc[:, ~df_new.columns.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b2528-e408-4c3c-bbe4-541dbcb4aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17897a64-5a96-408e-978c-bb20c8e560a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns again before scaling\n",
    "\n",
    "print(df_new.shape)\n",
    "cols_to_scale = df_new.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(df_new[cols_to_scale].shape)\n",
    "non_numeric_cols = df_new.columns.difference(cols_to_scale)\n",
    "print(df_new[non_numeric_cols].shape)\n",
    "print(df_new.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50adca39-bb1f-41e2-a4fe-c60527f20cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I can scale at last!\n",
    "# Identify numeric and non-numeric columns\n",
    "numeric_cols = df_new.select_dtypes(include=['int64', 'float64']).columns\n",
    "non_numeric_cols = df_new.columns.difference(numeric_cols)\n",
    "\n",
    "# Scale numeric columns\n",
    "scaler = RobustScaler()\n",
    "scaled_numeric = pd.DataFrame(\n",
    "    scaler.fit_transform(df_new[numeric_cols]),\n",
    "    columns=numeric_cols,\n",
    "    index=df_new.index\n",
    ")\n",
    "\n",
    "# Combine scaled numeric columns with non-numeric columns\n",
    "df_scaled = pd.concat([scaled_numeric, df_new[non_numeric_cols]], axis=1)\n",
    "\n",
    "# Quick check\n",
    "print(df_scaled.shape)\n",
    "print(df_scaled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a1f11-8b9b-4743-9516-8a652c00aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature names and duplicates before proceeding to feature assessment\n",
    "# List all column names\n",
    "print(df_scaled.columns.tolist())\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df_scaled.columns[df_scaled.columns.duplicated()].unique()\n",
    "print(\"Duplicate column names:\", duplicates)\n",
    "\n",
    "# Quick preview of new polynomial feature names\n",
    "poly_features = [col for col in df_scaled.columns if '^' in col or ':' in col]\n",
    "print(\"Sample polynomial features:\", poly_features[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d62502-5c7d-4daf-b6eb-a8bdd3f900e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skim features\n",
    "df_scaled.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efa0a81-01df-4cd2-927e-182fd0a48a7f",
   "metadata": {},
   "source": [
    "#### 5.5 Feature Assessment (Post-Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b22e09-8ce1-485d-acc9-b8ec79c597a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be9bc90-835f-4074-be1b-22e811697d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any column that starts with \"Order\" or contains \"Order\" in its name\n",
    "df_scaled = df_scaled.drop(\n",
    "    columns=[c for c in df_scaled.columns if \"Order\" in c]\n",
    ")\n",
    "\n",
    "print(\"Remaining columns:\", len(df_scaled.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e28d4-8a77-4141-9c07-7298a4fdc4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define original, created interaction terms and original capped features\n",
    "original_columns = ['PID', 'Lot Frontage', 'Overall Qual', 'Overall Cond', 'Year Built', 'Year Remod/Add', 'Exter Qual', 'Exter Cond', 'Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure', 'BsmtFin Type 1', 'BsmtFin SF 1', 'BsmtFin Type 2', 'BsmtFin SF 2', 'Bsmt Unf SF', 'Total Bsmt SF', 'Heating QC', '1st Flr SF', '2nd Flr SF', 'Low Qual Fin SF', 'Full Bath', 'Half Bath', 'Bedroom AbvGr', 'Kitchen AbvGr', 'Kitchen Qual', 'TotRms AbvGrd', 'Functional', 'Fireplaces', 'Fireplace Qu', 'Garage Yr Blt', 'Garage Area', 'Garage Qual', 'Garage Cond', 'Paved Drive', 'Mo Sold', 'Yr Sold', 'Bsmt Full Bath','Bsmt Half Bath','Garage Cars','SalePrice_bin', 'living_area_inconsistency','qual_group']\n",
    "interaction_columns = ['house_age', 'remodel_age', 'years_to_remodel', 'total_bathrooms', 'total_porch_area', 'total_bsmt_finished', 'living_area_ratio','avg_quality', 'qual_living_area_interaction']\n",
    "capped_features = ['Lot Area_capped','Mas Vnr Area_capped','Wood Deck SF_capped','Open Porch SF_capped','Enclosed Porch_capped', '3Ssn Porch_capped','Screen Porch_capped','Pool Area_capped','Misc Val_capped','Gr Liv Area_capped','SalePrice_capped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc253cdf-efcf-4896-83f8-1cef8d24af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one hot encoded features\n",
    "# OHE features = processed columns that weren't in the original raw dataset\n",
    "exclude_cols = original_columns + interaction_columns + capped_features\n",
    "\n",
    "\n",
    "ohe_features = [c for c in df_scaled.columns if df_scaled[c].nunique() == 2 and c not in exclude_cols]\n",
    "\n",
    "poly_features = [c for c in df_scaled.columns if c not in exclude_cols + ohe_features]\n",
    "\n",
    "print(\"Number of one-hot encoded features:\", len(ohe_features))\n",
    "print(\"Number of polynomial features:\", len(poly_features))\n",
    "print(ohe_features)\n",
    "print(poly_features)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de483f09-63f3-4a32-8483-45b708a314dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of original features:\", len(original_columns))\n",
    "print(\"Number of interaction items:\", len(interaction_columns))\n",
    "print(\"Number of capped features:\", len(capped_features))\n",
    "print(\"Number of one-hot encoded features:\", len(ohe_features))\n",
    "print(\"Number of polynomial features:\", len(poly_features))\n",
    "\n",
    "print(df_scaled.shape)\n",
    "print(len(original_columns) + len(interaction_columns) + len(capped_features) + len(ohe_features) + len(poly_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b255b-c42b-4401-8e03-6351bd58f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = original_columns + interaction_columns + capped_features + ohe_features + poly_features\n",
    "\n",
    "# Find which column(s) are missing or duplicated\n",
    "missing_in_df = [c for c in all_features if c not in df_scaled.columns]\n",
    "extra_in_df = [c for c in df_scaled.columns if c not in all_features]\n",
    "\n",
    "print(\"Columns counted but not in df_scaled:\", missing_in_df)\n",
    "print(\"Columns in df_scaled but not counted:\", extra_in_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a63b50-8b47-4fe2-b4ed-bd0afeaa857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check correlation with SalePrice_capped\n",
    "numeric_df = df_scaled.select_dtypes(include=['number'])\n",
    "corr_matrix = numeric_df.corr()\n",
    "saleprice_corr = corr_matrix['SalePrice_capped']\n",
    "\n",
    "# Filter correlations above threshold\n",
    "threshold = 0.5\n",
    "strong_corr = saleprice_corr[abs(saleprice_corr) > threshold].sort_values(key=abs, ascending=False)\n",
    "\n",
    "\n",
    "print(strong_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182cb685-17b1-4571-91c6-895bfbc237a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba451b-c860-4f72-a1d3-284eb635942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75041075-7340-4834-aea1-fb39efe148ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to features (exclude target)\n",
    "predictors = df_scaled[strong_corr.index.drop('SalePrice_capped')]\n",
    "\n",
    "# Compute correlation matrix among predictors\n",
    "pred_corr = predictors.corr().abs()\n",
    "\n",
    "# Find pairs with correlation > 0.8\n",
    "high_corr_pairs = (\n",
    "    pred_corr.where(np.triu(np.ones(pred_corr.shape), k=1).astype(bool))\n",
    "    .stack()\n",
    "    .reset_index()\n",
    "    .rename(columns={'level_0': 'Feature1', 'level_1': 'Feature2', 0: 'Correlation'})\n",
    ")\n",
    "\n",
    "high_corr_pairs = high_corr_pairs[high_corr_pairs['Correlation'] > 0.8]\n",
    "\n",
    "high_corr_pairs.sort_values('Correlation', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8b1c2f-36a4-4b06-81f6-d417f8b961b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_corr_pairs: DataFrame with columns Feature1, Feature2, Correlation\n",
    "# saleprice_corr: Series of correlations of all features with SalePrice_capped\n",
    "\n",
    "# 1. Determine which feature to keep in each highly collinear pair\n",
    "high_corr_pairs['Keep_feature'] = high_corr_pairs.apply(\n",
    "    lambda row: row['Feature1'] \n",
    "    if abs(saleprice_corr[row['Feature1']]) >= abs(saleprice_corr[row['Feature2']])\n",
    "    else row['Feature2'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 2. Add correlation of the kept feature with SalePrice_capped\n",
    "high_corr_pairs['Keep_feature_corr'] = high_corr_pairs['Keep_feature'].apply(lambda f: saleprice_corr[f])\n",
    "\n",
    "# 3. Optionally sort by collinearity correlation for reference\n",
    "high_corr_pairs = high_corr_pairs.sort_values('Correlation', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 4. Create a list of features to drop (the one NOT kept in each pair)\n",
    "high_corr_pairs['Drop_feature'] = high_corr_pairs.apply(\n",
    "    lambda row: row['Feature2'] if row['Keep_feature'] == row['Feature1'] else row['Feature1'], axis=1\n",
    ")\n",
    "\n",
    "# 5. Full list of features to drop (no duplicates)\n",
    "features_to_drop = high_corr_pairs['Drop_feature'].unique().tolist()\n",
    "\n",
    "# 6. Optional: inspect top rows\n",
    "high_corr_pairs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b47f9-e58c-4651-b82e-55e9b98b18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select only numeric columns for VIF calculation\n",
    "numeric_features = df_scaled.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Optionally, remove the target column\n",
    "numeric_features = [f for f in numeric_features if f != 'SalePrice_capped']\n",
    "\n",
    "# 2. Prepare the DataFrame for VIF calculation\n",
    "X_vif = df_scaled[numeric_features]\n",
    "\n",
    "# 3. Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = X_vif.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "\n",
    "# 4. Sort by VIF descending to see the most collinear features first\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 5. Flag features with high VIF (e.g., >5)\n",
    "vif_data['High_VIF'] = vif_data['VIF'] > 5\n",
    "\n",
    "vif_data.head(20)  # Show top 20 features with highest VIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764f31a-dbf6-455d-a989-b0c7a2128fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. Get the features to keep\n",
    "# ---------------------------\n",
    "\n",
    "# Keep all unique features selected in 'Keep_feature'\n",
    "features_to_keep = high_corr_pairs['Keep_feature'].unique().tolist()\n",
    "\n",
    "# Also include all features that weren't part of any high-correlation pair\n",
    "all_features = df_scaled.columns.tolist()\n",
    "features_in_pairs = high_corr_pairs[['Feature1','Feature2']].values.flatten().tolist()\n",
    "features_not_in_pairs = [f for f in all_features if f not in features_in_pairs]\n",
    "\n",
    "# Combine\n",
    "final_features = list(features_to_keep) + features_not_in_pairs\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Create the trimmed DataFrame\n",
    "# ---------------------------\n",
    "df_trimmed = df_scaled[final_features].copy()\n",
    "\n",
    "# Optional: check shape\n",
    "print(\"Original df shape:\", df_scaled.shape)\n",
    "print(\"Trimmed df shape:\", df_trimmed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1012bd7-d86c-4713-8f8e-d1626d1bd517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-check multicollinearity (VIF + Pearson)\n",
    "\n",
    "# --- VIF ---\n",
    "# Keep only numeric columns (in case some object slipped in)\n",
    "X_vif = df_trimmed.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "# Drop the target if still present\n",
    "if \"SalePrice_capped\" in X_vif.columns:\n",
    "    X_vif = X_vif.drop(columns=[\"SalePrice_capped\"])\n",
    "\n",
    "# Ensure no NaNs or inf\n",
    "X_vif = X_vif.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i)\n",
    "                   for i in range(X_vif.shape[1])]\n",
    "\n",
    "print(\"\\nTop 20 features by VIF (trimmed dataset):\")\n",
    "print(vif_data.sort_values(by=\"VIF\", ascending=False).head(20))\n",
    "\n",
    "# --- Pearson Correlation ---\n",
    "corr_matrix_trimmed = X_vif.corr().abs()\n",
    "\n",
    "threshold = 0.80\n",
    "high_corr_pairs_trimmed = (\n",
    "    corr_matrix_trimmed.where(np.triu(np.ones(corr_matrix_trimmed.shape), k=1).astype(bool))\n",
    "    .stack()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"Correlation\", \"level_0\": \"Feature1\", \"level_1\": \"Feature2\"})\n",
    ")\n",
    "\n",
    "high_corr_pairs_trimmed = high_corr_pairs_trimmed[\n",
    "    high_corr_pairs_trimmed[\"Correlation\"] > threshold\n",
    "].sort_values(by=\"Correlation\", ascending=False)\n",
    "\n",
    "print(f\"\\nNumber of highly correlated pairs > {threshold} in trimmed dataset:\", \n",
    "      len(high_corr_pairs_trimmed))\n",
    "print(high_corr_pairs_trimmed.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c210259-d8d4-4431-897a-1cee7dd8bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Summary of multicollinearity\n",
    "# -----------------------------\n",
    "\n",
    "# 1️⃣ High VIF features\n",
    "vif_threshold = 10.0  # adjust if you like\n",
    "high_vif_features = vif_data[vif_data['VIF'] > vif_threshold].copy()\n",
    "\n",
    "print(f\"Number of features with VIF > {vif_threshold}: {len(high_vif_features)}\")\n",
    "print(high_vif_features[['Feature','VIF']].head(20))\n",
    "\n",
    "# 2️⃣ Highly correlated pairs (Pearson)\n",
    "corr_threshold = 0.8\n",
    "high_corr_pairs_summary = high_corr_pairs[high_corr_pairs['Correlation'] > corr_threshold].copy()\n",
    "\n",
    "print(f\"\\nNumber of highly correlated pairs with correlation > {corr_threshold}: {len(high_corr_pairs_summary)}\")\n",
    "print(high_corr_pairs_summary[['Feature1','Feature2','Correlation','Keep_feature']].head(20))\n",
    "\n",
    "# 3️⃣ Optional: combine info into one DataFrame for reference\n",
    "high_corr_vif_summary = high_corr_pairs_summary.copy()\n",
    "high_corr_vif_summary['Keep_feature_VIF'] = high_corr_vif_summary['Keep_feature'].map(\n",
    "    dict(zip(vif_data['Feature'], vif_data['VIF']))\n",
    ")\n",
    "high_corr_vif_summary['Drop_feature_VIF'] = high_corr_vif_summary['Drop_feature'].map(\n",
    "    dict(zip(vif_data['Feature'], vif_data['VIF']))\n",
    ")\n",
    "\n",
    "high_corr_vif_summary.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffec7f9-79f6-40b4-8eef-4236b17b8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# PARAMETERS\n",
    "# ----------------------------\n",
    "vif_threshold = 10.0\n",
    "target_col = \"SalePrice_capped\"\n",
    "\n",
    "# ----------------------------\n",
    "# INITIAL SETUP\n",
    "# ----------------------------\n",
    "df_pruned = df_trimmed.copy()  # Work on trimmed dataset\n",
    "numeric_features = df_pruned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_features = [f for f in numeric_features if f != target_col]\n",
    "\n",
    "# Compute VIFs once\n",
    "X_vif = df_pruned[numeric_features].replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "\n",
    "# Identify features exceeding VIF threshold\n",
    "high_vif_features = vif_data[vif_data[\"VIF\"] > vif_threshold]\n",
    "print(f\"Number of features with VIF > {vif_threshold}: {len(high_vif_features)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# DECIDE WHICH FEATURES TO DROP\n",
    "# ----------------------------\n",
    "features_to_drop = []\n",
    "\n",
    "if 'saleprice_corr' in globals():\n",
    "    # Keep the feature most correlated with the target in each highly collinear pair\n",
    "    for feature in high_vif_features[\"Feature\"]:\n",
    "        if feature not in features_to_drop:\n",
    "            # Identify all features highly correlated with this one (optional: threshold 0.8)\n",
    "            corr_with_others = df_pruned[numeric_features].corr()[feature].abs()\n",
    "            correlated_features = corr_with_others[corr_with_others > 0.8].index.tolist()\n",
    "            \n",
    "            # Among these, keep the one most correlated with target\n",
    "            corr_target = {f: abs(saleprice_corr.get(f, 0)) for f in correlated_features}\n",
    "            feature_to_keep = max(corr_target, key=corr_target.get)\n",
    "            \n",
    "            # Drop the rest\n",
    "            for f in correlated_features:\n",
    "                if f != feature_to_keep and f not in features_to_drop:\n",
    "                    features_to_drop.append(f)\n",
    "else:\n",
    "    # Simple approach: drop all high-VIF features\n",
    "    features_to_drop = high_vif_features[\"Feature\"].tolist()\n",
    "\n",
    "# ----------------------------\n",
    "# CREATE PRUNED DATAFRAME\n",
    "# ----------------------------\n",
    "df_pruned = df_pruned.drop(columns=features_to_drop)\n",
    "print(f\"Features removed: {len(features_to_drop)}\")\n",
    "print(\"Top 20 removed features:\", features_to_drop[:20])\n",
    "print(\"Final dataframe shape:\", df_pruned.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ffad51-4d52-421f-96a1-e043ad5849b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pruned.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf2780-0226-4bd0-854d-322fcf0cec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-Check correlation with SalePrice_capped\n",
    "# Convert bools to integers for correlation\n",
    "df_corr = df_pruned.copy()\n",
    "bool_cols = df_corr.select_dtypes(include='bool').columns\n",
    "df_corr[bool_cols] = df_corr[bool_cols].astype(int)\n",
    "\n",
    "# Now select all numeric features including converted bools\n",
    "numeric_df = df_corr.select_dtypes(include=['number'])\n",
    "\n",
    "# Compute correlation with target\n",
    "saleprice_corr = numeric_df.corr()['SalePrice_capped'].drop('SalePrice_capped').sort_values(key=abs, ascending=False)\n",
    "print(saleprice_corr)\n",
    "print(len(saleprice_corr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe0995-b320-43aa-895b-dc5c576ace82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set correlation threshold\n",
    "threshold = 0.1  # adjust as needed (e.g., 0.1, 0.3, 0.5)\n",
    "\n",
    "# Filter features whose absolute correlation with target is above threshold\n",
    "strong_corr = saleprice_corr[abs(saleprice_corr) > threshold]\n",
    "\n",
    "# Optional: convert to DataFrame for easier inspection\n",
    "strong_corr_df = pd.DataFrame({\n",
    "    'Feature': strong_corr.index,\n",
    "    'Correlation': strong_corr.values\n",
    "}).sort_values(by='Correlation', key=abs, ascending=False)\n",
    "\n",
    "print(strong_corr_df)\n",
    "print(f\"Number of features above threshold {threshold}: {len(strong_corr_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1042e-a3a8-4c7a-b169-f933441b58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update final df to have the top 100 features\n",
    "\n",
    "# Get list of selected features\n",
    "selected_features = strong_corr_df['Feature'].tolist()\n",
    "\n",
    "# Ensure target is in the final dataset\n",
    "if 'SalePrice_capped' not in selected_features:\n",
    "    selected_features.append('SalePrice_capped')\n",
    "\n",
    "# Create final dataset with target included\n",
    "df_final = df_pruned[selected_features]\n",
    "\n",
    "# Separate features and target for modeling\n",
    "X_final = df_final.drop('SalePrice_capped', axis=1)\n",
    "y_final = df_final['SalePrice_capped']\n",
    "\n",
    "\n",
    "# Create final dataset\n",
    "df_final = df_pruned[selected_features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd32c1-149b-4bc0-8fac-daf7fe8a80d7",
   "metadata": {},
   "source": [
    "## 6. Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6768dea2-be57-49e8-ac19-444d72e5f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = [\n",
    "    'qual_living_area_interaction',\n",
    "    'Overall Qual Exter Qual Fireplace Qu',\n",
    "    'Overall Qual^2 BsmtFin Type 1',\n",
    "    'total_bathrooms',\n",
    "    'Total Bsmt SF'\n",
    "]\n",
    "\n",
    "# Split dataset once\n",
    "X_all = df_final[top_features]\n",
    "y_all = df_final['SalePrice_capped']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "for feature in top_features:\n",
    "    # Prepare training and test sets for this feature\n",
    "    X_train_feat = sm.add_constant(X_train[[feature]])\n",
    "    X_test_feat = sm.add_constant(X_test[[feature]])\n",
    "    \n",
    "    # Fit SLR\n",
    "    model = sm.OLS(y_train, X_train_feat).fit()\n",
    "    \n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train_feat)\n",
    "    y_test_pred = model.predict(X_test_feat)\n",
    "    \n",
    "    # Compute metrics\n",
    "    results.append({\n",
    "        'Feature': feature,\n",
    "        'Train R2': r2_score(y_train, y_train_pred),\n",
    "        'Test R2': r2_score(y_test, y_test_pred),\n",
    "        'Train RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'Test RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        'Train MAE': np.mean(np.abs(y_train - y_train_pred)),\n",
    "        'Test MAE': np.mean(np.abs(y_test - y_test_pred))\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa118b3-bef5-485f-adb0-88d7cb7e33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure directory exists to save plots\n",
    "output_dir = \"model1_diagnostics\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for feature in top_features:\n",
    "    # Prepare training data\n",
    "    X_train_feat = sm.add_constant(X_train[[feature]])\n",
    "    model = sm.OLS(y_train, X_train_feat).fit()\n",
    "    \n",
    "    # Residuals and fitted values\n",
    "    residuals = model.resid\n",
    "    fitted = model.fittedvalues\n",
    "    \n",
    "    # -------------------------\n",
    "    # 1. Residuals vs Fitted\n",
    "    # -------------------------\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(fitted, residuals, alpha=0.5)\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.xlabel(\"Fitted values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(f\"{feature}: Residuals vs Fitted\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{feature}_residuals_vs_fitted.png\")  # Save first\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # -------------------------\n",
    "    # 2. QQ plot\n",
    "    # -------------------------\n",
    "    fig = sm.qqplot(residuals, line='45', fit=True)\n",
    "    plt.title(f\"{feature}: QQ Plot\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{feature}_qqplot.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # -------------------------\n",
    "    # 3. Cook's distance\n",
    "    # -------------------------\n",
    "    influence = model.get_influence()\n",
    "    cooks_d, _ = influence.cooks_distance\n",
    "    \n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.stem(range(len(cooks_d)), cooks_d, markerfmt=\",\")\n",
    "    plt.xlabel(\"Observation index\")\n",
    "    plt.ylabel(\"Cook's distance\")\n",
    "    plt.title(f\"{feature}: Cook's Distance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{feature}_cooks_distance.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # -------------------------\n",
    "    # 4. Influence plot\n",
    "    # -------------------------\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    sm.graphics.influence_plot(model, ax=ax, criterion=\"cooks\")\n",
    "    plt.title(f\"{feature}: Influence Plot\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{feature}_influence_plot.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # -------------------------\n",
    "    # 5. Normality tests\n",
    "    # -------------------------\n",
    "    shapiro_test = stats.shapiro(residuals)\n",
    "    jb_test = sm.stats.stattools.jarque_bera(residuals)\n",
    "    \n",
    "    print(f\"{feature} Diagnostics:\")\n",
    "    print(f\"  Shapiro-Wilk test: W={shapiro_test[0]:.4f}, p={shapiro_test[1]:.4f}\")\n",
    "    print(f\"  Jarque-Bera test: JB={jb_test[0]:.4f}, p={jb_test[1]:.4f}\")\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66adc76e-a9a3-4a12-beba-f2c50a025193",
   "metadata": {},
   "source": [
    "## 7. Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4066cc-fd3d-4c98-a154-eee6fde02ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Define scorers for CV\n",
    "# --------------------------\n",
    "scoring = {\n",
    "    'r2': make_scorer(r2_score),\n",
    "    'rmse': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "    'mae': make_scorer(mean_absolute_error)\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# Features & target\n",
    "# --------------------------\n",
    "# Drop problematic / leakage columns\n",
    "X = df_final.drop('SalePrice_capped', axis=1)\n",
    "\n",
    "# Target\n",
    "y = df_final['SalePrice_capped']\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 5-fold CV with Multiple Linear Regression\n",
    "# --------------------------\n",
    "linreg = LinearRegression()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    linreg, X, y,\n",
    "    cv=kf,\n",
    "    scoring=scoring,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Summarise CV results\n",
    "# --------------------------\n",
    "results_df = pd.DataFrame({\n",
    "    'Train R2': cv_results['train_r2'],\n",
    "    'Test R2': cv_results['test_r2'],\n",
    "    'Train RMSE': cv_results['train_rmse'],\n",
    "    'Test RMSE': cv_results['test_rmse'],\n",
    "    'Train MAE': cv_results['train_mae'],\n",
    "    'Test MAE': cv_results['test_mae']\n",
    "})\n",
    "\n",
    "print(\"Cross-validation results (per fold):\")\n",
    "print(results_df)\n",
    "\n",
    "print(\"\\nAverage performance across folds:\")\n",
    "print(results_df.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf7355d-470e-4f84-af82-934d35e9ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise CV results\n",
    "# Extract per-fold values\n",
    "folds = np.arange(1, 6)\n",
    "train_r2 = results_df['Train R2']\n",
    "test_r2 = results_df['Test R2']\n",
    "train_rmse = results_df['Train RMSE']\n",
    "test_rmse = results_df['Test RMSE']\n",
    "train_mae = results_df['Train MAE']\n",
    "test_mae = results_df['Test MAE']\n",
    "\n",
    "output_dir = \"model2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ---------- R² ----------\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(folds - 0.15, train_r2, width=0.3, label='Train R²')\n",
    "plt.bar(folds + 0.15, test_r2, width=0.3, label='Test R²')\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.ylabel(\"R²\")\n",
    "plt.title(\"Cross-validation R² per Fold\")\n",
    "plt.xticks(folds)\n",
    "plt.ylim(0,1)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/cv_r2_per_fold.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ---------- RMSE ----------\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(folds, train_rmse, marker='o', label='Train RMSE')\n",
    "plt.plot(folds, test_rmse, marker='o', label='Test RMSE')\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Cross-validation RMSE per Fold\")\n",
    "plt.xticks(folds)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/cv_rmse_per_fold.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ---------- MAE ----------\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(folds, train_mae, marker='o', label='Train MAE')\n",
    "plt.plot(folds, test_mae, marker='o', label='Test MAE')\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"Cross-validation MAE per Fold\")\n",
    "plt.xticks(folds)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/cv_mae_per_fold.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0db2fc-f560-4319-a0ea-2d6f32bb6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the important features\n",
    "\n",
    "# --------------------------\n",
    "# Fit Linear Regression on full data\n",
    "# --------------------------\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# --------------------------\n",
    "# Feature importance (coefficients)\n",
    "# --------------------------\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': linreg.coef_\n",
    "})\n",
    "\n",
    "top_model_2_features = feature_importance_df.reindex(\n",
    "    feature_importance_df['Coefficient'].abs().sort_values(ascending=False).index\n",
    ").head(10)\n",
    "\n",
    "print(top_model_2_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a11374-bf03-4f6e-abc1-f9d424ccf5a4",
   "metadata": {},
   "source": [
    "## 8. Iterative Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6eb489-feb2-4103-965e-12b1277495ce",
   "metadata": {},
   "source": [
    "#### 8.1 Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67470b0f-e57f-4471-85aa-84c7301da091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Setup output folder\n",
    "# --------------------------\n",
    "out_dir = \"outlier_treatment_29_08_25\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# Prepare data for OLS (full-sample)\n",
    "# --------------------------\n",
    "X_sm = df_final.drop('SalePrice_capped', axis=1).copy()\n",
    "\n",
    "# Convert boolean columns to int, drop near-constant ones\n",
    "bool_cols = X_sm.select_dtypes(include='bool').columns\n",
    "for col in bool_cols:\n",
    "    freq = X_sm[col].value_counts(normalize=True).max()\n",
    "    if freq > 0.95:\n",
    "        X_sm.drop(columns=col, inplace=True)\n",
    "    else:\n",
    "        X_sm[col] = X_sm[col].astype(int)\n",
    "\n",
    "# Ensure all numeric\n",
    "X_sm = X_sm.apply(pd.to_numeric, errors='coerce')\n",
    "X_sm = sm.add_constant(X_sm)\n",
    "\n",
    "# Target\n",
    "y_sm = pd.to_numeric(df_final['SalePrice_capped'], errors='coerce')\n",
    "\n",
    "# Drop any rows with NaN\n",
    "mask = X_sm.notna().all(axis=1) & y_sm.notna()\n",
    "X_sm = X_sm.loc[mask]\n",
    "y_sm = y_sm.loc[mask]\n",
    "\n",
    "# --------------------------\n",
    "# Fit full-sample OLS\n",
    "# --------------------------\n",
    "ols_cook_distance1 = sm.OLS(y_sm, X_sm).fit()\n",
    "\n",
    "# Influence measures\n",
    "influence = ols_cook_distance1.get_influence()\n",
    "cooks_d, _ = influence.cooks_distance\n",
    "student_resid = influence.resid_studentized_external\n",
    "leverage = influence.hat_matrix_diag\n",
    "\n",
    "# --------------------------\n",
    "# Identify influential points\n",
    "# --------------------------\n",
    "n = X_sm.shape[0]\n",
    "threshold = 4/n\n",
    "influential_points = np.where(cooks_d > threshold)[0]\n",
    "\n",
    "# Select top 10 automatically\n",
    "top_10_indices = np.argsort(cooks_d)[-10:][::-1]\n",
    "top_10_index_values = X_sm.index[top_10_indices]\n",
    "\n",
    "top_10_df = pd.DataFrame({\n",
    "    'index': top_10_index_values,\n",
    "    \"Cook's Distance\": cooks_d[top_10_indices],\n",
    "    'target': y_sm.loc[top_10_index_values].values\n",
    "})\n",
    "\n",
    "print(f\"Threshold for Cook's distance: {threshold:.4f}\")\n",
    "print(f\"Number of influential points: {len(influential_points)}\")\n",
    "print(\"Top 10 influential points (index, Cook's D, target):\")\n",
    "print(top_10_df)\n",
    "\n",
    "# --------------------------\n",
    "# Bubble plot: leverage vs studentized residuals\n",
    "# --------------------------\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "bubble_size = 500 * (cooks_d / np.nanmax(cooks_d))\n",
    "ax.scatter(leverage, student_resid, s=bubble_size, alpha=0.5)\n",
    "\n",
    "# Convert to Series for indexed selection\n",
    "leverage_series = pd.Series(leverage, index=X_sm.index)\n",
    "student_resid_series = pd.Series(student_resid, index=X_sm.index)\n",
    "\n",
    "# Highlight top 10\n",
    "ax.scatter(leverage_series.loc[top_10_index_values],\n",
    "           student_resid_series.loc[top_10_index_values],\n",
    "           color='red', s=100, label='Top influencers')\n",
    "\n",
    "ax.axhline(y=0, color='grey', linestyle='--')\n",
    "ax.set_xlabel(\"Leverage\")\n",
    "ax.set_ylabel(\"Studentized Residuals\")\n",
    "ax.set_title(\"Influence Plot (Bubble Size = Cook's Distance)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{out_dir}/influence_plot.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8593c2-e7eb-46c6-ab77-d87a77377eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This appears unstable so checking VIF\n",
    "# Drop constant column if present\n",
    "X_numeric = X_sm.drop(columns='const', errors='ignore')\n",
    "\n",
    "# Only numeric predictors\n",
    "X_numeric = X_numeric.select_dtypes(include=np.number)\n",
    "\n",
    "# Compute VIFs\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X_numeric.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_numeric.values, i)\n",
    "                   for i in range(X_numeric.shape[1])]\n",
    "\n",
    "# Sort by VIF descending\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "# Show high VIFs (commonly > 5 or > 10)\n",
    "high_vif = vif_data[vif_data['VIF'] > 5]\n",
    "\n",
    "print(\"Features with high multicollinearity (VIF > 5):\")\n",
    "print(high_vif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5322faf4-cb45-4cc4-9821-f5b5f136579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create dummies\n",
    "X_sm = pd.get_dummies(\n",
    "    df_final.drop(columns='SalePrice_capped', axis=1),\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# Step 2: Force numeric float type\n",
    "X_sm = X_sm.astype(float)\n",
    "\n",
    "# Step 3: Drop any rows with NaN or inf\n",
    "X_sm = X_sm.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
    "\n",
    "# Step 4: Drop near-constant columns\n",
    "near_constant_cols = [col for col in X_sm.columns \n",
    "                      if X_sm[col].value_counts(normalize=True).max() > 0.95]\n",
    "X_sm.drop(columns=near_constant_cols, inplace=True)\n",
    "\n",
    "# Step 5: Optional: remove any remaining object columns just in case\n",
    "X_sm = X_sm.select_dtypes(include=[np.number])\n",
    "\n",
    "# Step 6: Compute VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X_sm.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_sm.values, i) \n",
    "                   for i in range(X_sm.shape[1])]\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "print(\"Top 20 VIFs:\")\n",
    "print(vif_data.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d865cb1f-65b3-4d85-a307-eee476dd6938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant numeric columns\n",
    "numeric_drop = ['Full Bath', 'Half Bath', 'Bsmt Full Bath']\n",
    "X_sm = X_sm.drop(columns=numeric_drop)\n",
    "\n",
    "#Drop redundant dummy columns\n",
    "for col in X_sm.columns:\n",
    "    if X_sm[col].value_counts(normalize=True).max() > 0.95:\n",
    "        X_sm.drop(columns=col, inplace=True)\n",
    "\n",
    "# Drop one of any pair of perfectly correlated columns\n",
    "corr_matrix = X_sm.corr().abs()\n",
    "high_corr_pairs = [(corr_matrix.columns[i], corr_matrix.columns[j])\n",
    "                   for i, j in zip(*np.where(corr_matrix>0.99)) if i<j]\n",
    "for col1, col2 in high_corr_pairs:\n",
    "    X_sm.drop(columns=col2, inplace=True)\n",
    "\n",
    "# Ensure all columns are numeric and no NaNs\n",
    "X_sm = X_sm.apply(pd.to_numeric, errors='coerce')\n",
    "X_sm = X_sm.dropna(axis=1, how='all')\n",
    "X_sm = X_sm.dropna(axis=0)\n",
    "\n",
    "# Recompute VIFs\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X_sm.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_sm.values, i) for i in range(X_sm.shape[1])]\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "print(vif_data.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46863308-3794-4c14-bce0-c5136ab703a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further VIF clearning needed\n",
    "\n",
    "drop_features = [\n",
    "    'years_to_remodel',\n",
    "    'Wood Deck SF_capped', 'Open Porch SF_capped',\n",
    "    'Exterior 1st_MetalSd', 'Exterior 2nd_MetalSd',\n",
    "    'Garage Type_Detchd',\n",
    "    'Garage Cars Mo Sold avg_quality', 'Garage Cars Bsmt Unf SF avg_quality',\n",
    "    'Roof Style_Gable',\n",
    "    'Sale Condition_Partial'\n",
    "]\n",
    "\n",
    "X_sm_cleaned = X_sm.drop(columns=[f for f in drop_features if f in X_sm.columns])\n",
    "\n",
    "# Recalculate VIFs after cleaning\n",
    "def calculate_vif(df):\n",
    "    return pd.DataFrame({\n",
    "        'feature': df.columns,\n",
    "        'VIF': [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    }).sort_values('VIF', ascending=False)\n",
    "\n",
    "vif_df_cleaned = calculate_vif(X_sm_cleaned)\n",
    "print(\"Top VIFs after manual grouping cleanup:\")\n",
    "print(vif_df_cleaned.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7fc812-566a-4b49-af3a-6a08ff7a6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try to run cook's Distance again\n",
    "\n",
    "# --------------------------\n",
    "# Setup output folder\n",
    "# --------------------------\n",
    "out_dir = \"outlier_treatment_29_08_25\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# Use your cleaned X_sm and y_sm\n",
    "# --------------------------\n",
    "# X_sm: your VIF-cleaned dataframe (numeric, dummies, no near-constant columns)\n",
    "# y_sm: df_pruned['SalePrice_capped'], numeric\n",
    "X_sm = X_sm_cleaned\n",
    "# Ensure numeric and add constant\n",
    "X_sm = sm.add_constant(X_sm)\n",
    "y_sm = pd.to_numeric(y_sm, errors='coerce')\n",
    "\n",
    "# Drop rows with any NaN (just in case)\n",
    "mask = X_sm.notna().all(axis=1) & y_sm.notna()\n",
    "X_sm = X_sm.loc[mask]\n",
    "y_sm = y_sm.loc[mask]\n",
    "\n",
    "# --------------------------\n",
    "# Fit OLS\n",
    "# --------------------------\n",
    "ols_cook_distance2 = sm.OLS(y_sm, X_sm).fit()\n",
    "\n",
    "# --------------------------\n",
    "# Influence measures\n",
    "# --------------------------\n",
    "influence = ols_cook_distance2.get_influence()\n",
    "cooks_d, _ = influence.cooks_distance\n",
    "student_resid = influence.resid_studentized_external\n",
    "leverage = influence.hat_matrix_diag\n",
    "\n",
    "# --------------------------\n",
    "# Flag influential points\n",
    "# --------------------------\n",
    "n = X_sm.shape[0]\n",
    "threshold = 4/n\n",
    "influential_points = np.where(cooks_d > threshold)[0]\n",
    "\n",
    "print(f\"Threshold for Cook's distance: {threshold:.4f}\")\n",
    "print(f\"Number of influential points: {len(influential_points)}\")\n",
    "\n",
    "# Get top 10 influential points\n",
    "top_10_indices = np.argsort(cooks_d)[-10:][::-1]\n",
    "top_10_df = pd.DataFrame({\n",
    "    'index': X_sm.index[top_10_indices],\n",
    "    \"Cook's Distance\": cooks_d[top_10_indices],\n",
    "    'target': y_sm.iloc[top_10_indices].values\n",
    "})\n",
    "print(\"Top 10 influential points (index, Cook's D, target):\")\n",
    "print(top_10_df)\n",
    "\n",
    "# --------------------------\n",
    "# Bubble plot: leverage vs studentized residuals\n",
    "# --------------------------\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "bubble_size = 500 * (cooks_d / np.nanmax(cooks_d))\n",
    "ax.scatter(leverage, student_resid, s=bubble_size, alpha=0.5)\n",
    "\n",
    "# Convert arrays to Series for indexed selection\n",
    "leverage_series = pd.Series(leverage, index=X_sm.index)\n",
    "student_resid_series = pd.Series(student_resid, index=X_sm.index)\n",
    "\n",
    "# Highlight top 10 influencers\n",
    "ax.scatter(leverage_series.loc[top_10_df['index']],\n",
    "           student_resid_series.loc[top_10_df['index']],\n",
    "           color='red', s=100, label='Top influencers')\n",
    "\n",
    "ax.axhline(y=0, color='grey', linestyle='--')\n",
    "ax.set_xlabel(\"Leverage\")\n",
    "ax.set_ylabel(\"Studentized Residuals\")\n",
    "ax.set_title(\"Influence Plot (Bubble Size = Cook's Distance)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{out_dir}/influence_plot.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc7225-21df-45fe-ae64-3834da9b633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at cooks distance once more alongside key diagnostics for (as yet unchanged) Model2 to provide baseline\n",
    "\n",
    "# Output folder\n",
    "out_dir = \"outlier_treatment3_29_08_25\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Prepare data (VIF-cleaned, numeric)\n",
    "X_infl = X_sm.apply(pd.to_numeric, errors='coerce')\n",
    "y_infl = pd.to_numeric(y_sm, errors='coerce')\n",
    "\n",
    "# Drop NaN / Inf rows\n",
    "mask = X_infl.notna().all(axis=1) & np.isfinite(X_infl).all(axis=1) & y_infl.notna() & np.isfinite(y_infl)\n",
    "X_infl = X_infl.loc[mask]\n",
    "y_infl = y_infl.loc[mask]\n",
    "\n",
    "# Add constant\n",
    "X_infl = sm.add_constant(X_infl)\n",
    "\n",
    "# Fit OLS\n",
    "ols_cook_distance4 = sm.OLS(y_infl, X_infl).fit()\n",
    "\n",
    "# Influence measures\n",
    "influence = ols_cook_distance4.get_influence()\n",
    "cooks_d, _ = influence.cooks_distance\n",
    "student_resid = influence.resid_studentized_external\n",
    "leverage = influence.hat_matrix_diag\n",
    "\n",
    "# Identify top influencers\n",
    "threshold = 4 / X_infl.shape[0]\n",
    "influential_points = np.where(cooks_d > threshold)[0]\n",
    "\n",
    "top_10_idx = np.argsort(cooks_d)[-10:][::-1]\n",
    "top_10_df = pd.DataFrame({\n",
    "    'index': X_infl.index[top_10_idx],\n",
    "    \"Cook's Distance\": cooks_d[top_10_idx],\n",
    "    'target': y_infl.iloc[top_10_idx].values\n",
    "})\n",
    "\n",
    "print(f\"Cook's distance threshold: {threshold:.4f}\")\n",
    "print(f\"Number of influential points: {len(influential_points)}\")\n",
    "print(\"Top 10 influential points:\")\n",
    "print(top_10_df)\n",
    "\n",
    "# Bubble plot\n",
    "plt.figure(figsize=(8,6))\n",
    "bubble_size = 500 * (cooks_d / np.nanmax(cooks_d))\n",
    "plt.scatter(leverage, student_resid, s=bubble_size, alpha=0.5)\n",
    "plt.scatter(leverage[top_10_idx], student_resid[top_10_idx], color='red', s=100, label='Top influencers')\n",
    "plt.axhline(0, color='grey', linestyle='--')\n",
    "plt.xlabel(\"Leverage\")\n",
    "plt.ylabel(\"Studentized Residuals\")\n",
    "plt.title(\"Influence Plot (Cook's Distance)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{out_dir}/influence_plot.png\")\n",
    "plt.show()\n",
    "\n",
    "# MODEL 2 EVALUATION\n",
    "\n",
    "# Features & target (same as model 2)\n",
    "X_cv = df_final.drop('SalePrice_capped', axis=1)\n",
    "\n",
    "# Target\n",
    "y_cv = df_final['SalePrice_capped']\n",
    "\n",
    "# Scorers for CV\n",
    "scoring = {\n",
    "    'r2': make_scorer(r2_score),\n",
    "    'rmse': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "    'mae': make_scorer(mean_absolute_error)\n",
    "}\n",
    "\n",
    "linreg = LinearRegression()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = cross_validate(linreg, X_cv, y_cv, cv=kf, scoring=scoring, return_train_score=True)\n",
    "\n",
    "results_df2 = pd.DataFrame({\n",
    "    'Train R2': cv_results['train_r2'],\n",
    "    'Test R2': cv_results['test_r2'],\n",
    "    'Train RMSE': cv_results['train_rmse'],\n",
    "    'Test RMSE': cv_results['test_rmse'],\n",
    "    'Train MAE': cv_results['train_mae'],\n",
    "    'Test MAE': cv_results['test_mae']\n",
    "})\n",
    "\n",
    "print(\"Cross-validation results (per fold):\")\n",
    "print(results_df2)\n",
    "print(\"\\nAverage performance across folds:\")\n",
    "print(results_df2.mean())\n",
    "\n",
    "# Residual Diagnostics for model 2\n",
    "# Fit on full data for residual plots\n",
    "linreg.fit(X_cv, y_cv)\n",
    "y_pred = linreg.predict(X_cv)\n",
    "residuals = y_cv - y_pred\n",
    "\n",
    "# Residuals vs Fitted\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(0, color='grey', linestyle='--')\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Fitted (Model 2)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{out_dir}/residuals_vs_fitted_model2.png\") \n",
    "plt.show()\n",
    "\n",
    "# QQ Plot\n",
    "plt.figure(figsize=(6,6))\n",
    "sm.qqplot(residuals, line='45', fit=True)\n",
    "plt.title(\"QQ Plot of Residuals (Model 2)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"f\"{out_dir}/qq_plot_model2.png\")\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk test\n",
    "from scipy import stats\n",
    "shapiro_p = stats.shapiro(residuals)[1]\n",
    "print(f\"Shapiro-Wilk p-value (Model 2 residuals): {shapiro_p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67541b79-9090-40c8-b56b-71cd21f28e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only numeric columns\n",
    "X_vif = X_cv.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X_vif.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_vif.values, i) \n",
    "                   for i in range(X_vif.shape[1])]\n",
    "\n",
    "# Round and sort\n",
    "vif_data['VIF'] = vif_data['VIF'].round(3)\n",
    "vif_data_sorted = vif_data.sort_values('VIF', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(vif_data_sorted.head(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645efef4-4552-40c9-87a3-0252e6f92a38",
   "metadata": {},
   "source": [
    "\n",
    "###### Summary\n",
    "\n",
    "1. Residuals vs Fitted\n",
    "Elliptical but denser on the left → mild heteroscedasticity.\n",
    "Slight spread at higher fitted values, with ~5 outliers between 1–4 (fitted) and -1.5 to -3.8 (residuals), and ~10 points between 3–4 (fitted) and -0.5 to 0.5 (residuals). Overall pattern typical for skewed real estate targets.\n",
    "\n",
    "2. QQ Plot & Shapiro-Wilk\n",
    "QQ plot: Most points near the line at upper quantiles, slight right skew. Lower quantiles more spread than before, two extreme points at (-3, -13).\n",
    "Shapiro-Wilk p-value ≈ 0 → residuals not perfectly normal. Large dataset → minor deviations unlikely to harm prediction.\n",
    "\n",
    "3. Cook’s Distance\n",
    "Approximately 156 points exceed the 4/n threshold (0.0014), indicating a sizable number of potentially influential observations.\n",
    "\n",
    "Top Cook’s D values are dominated by extreme residuals or leverage points; the largest Cook’s D is ≈ 0.887.\n",
    "\n",
    "The most influential points (e.g., indices 1498, 2180, 2181) may disproportionately affect coefficient estimates, suggesting the need to investigate or possibly Winsorize/extensively check these rows.\n",
    "\n",
    "Overall, while many points are flagged, the majority have moderate influence (Cook’s D < 0.05), so the model is not dominated by a few extreme cases.\n",
    "\n",
    "4. VIFs\n",
    "Several features exhibit very high multicollinearity: years_to_remodel, remodel_age, and house_age have VIF = ∞, indicating perfect or near-perfect linear dependence.\n",
    "\n",
    "Other notable high-VIF features include total_bathrooms (50.77), total_porch_area (41.14), Wood Deck SF_capped (29.44), and Full Bath (26.13), highlighting remaining multicollinearity concerns.\n",
    "\n",
    "Features with VIF > 10 (e.g., Bsmt Full Bath, qual_living_area_interaction, Garage Cars Mo Sold avg_quality) suggest some coefficients may be unstable or inflated.\n",
    "\n",
    "Despite preprocessing and feature pruning, some collinearity persists; careful interpretation of coefficients is advised, and future regularization may help stabilize the model.\n",
    "\n",
    "\n",
    "Implication: Many features remain highly collinear, with several showing VIFs well above 10 and some even infinite. For predictive purposes, this may be tolerable, but coefficient estimates for these variables are unstable. If interpretability becomes a priority, consider combining, dropping, or transforming highly collinear columns.\n",
    "\n",
    "\n",
    "###### Interpretation\n",
    "\n",
    "Model R² > 90% already → likely any removal of points will not improve predictive accuracy much.\n",
    "\n",
    "Cook’s distance shows influential points, but removing hundreds can destroy model structure\n",
    "\n",
    "Residuals indicate some skew and heteroscedasticity → could consider robust regression or transformation (e.g., log(y)) in future projects, but out of scope if sticking to vanilla linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74f986-4107-429e-8a3c-3c9f92ea777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose top_cooks_indices is a list or array of indices of top Cook's distances\n",
    "top_cooks_indices = [1498, 2180, 2181, 1182, 2737, 2570, 1945, 1782, 91, 2666]\n",
    "\n",
    "# Define the columns you want to display\n",
    "cols_to_show = ['Gr Liv Area_capped', 'Total Bsmt SF', 'Overall Qual', 'Overall Cond', \n",
    "                'SalePrice_capped', 'Year Built', 'Year Remod/Add', 'Lot Area_capped', \n",
    "                'BsmtFin SF 1', 'BsmtFin SF 2', 'Garage Cars', 'Garage Area']\n",
    "\n",
    "# Select the rows and only these columns\n",
    "top_cooks_original = df_new.loc[top_cooks_indices, cols_to_show]\n",
    "\n",
    "# Display them nicely\n",
    "display(\n",
    "    top_cooks_original.style\n",
    "    .set_table_attributes(\"style='display:inline'\")  # inline display\n",
    "    .set_table_styles([{'selector': 'th', 'props': [('font-size', '12pt')]}])  # header style\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a40d6-0549-40e0-b2bf-f8b092312975",
   "metadata": {},
   "source": [
    "Let's check for anything unusual in these rows.  \n",
    "\n",
    "1498: Huge home, top quality, but extremely low sale price → likely data error or extreme discount.\n",
    "\n",
    "2180: Very large home, high quality, low sale price → potential outlier.\n",
    "\n",
    "2181: Large home, high quality, low price → unusual, similar concern as 2180.\n",
    "\n",
    "1182: Large home, high quality but poor condition → low price plausible but extreme.\n",
    "\n",
    "2737: Large home, moderate quality but very high condition → price relatively high for quality.\n",
    "\n",
    "2570: Large home, decent quality, low basement area → sale price slightly high for features.\n",
    "\n",
    "1945: Large home, modest quality, minimal basement → price seems inconsistent, potential outlier.\n",
    "\n",
    "1782: Smaller home, decent quality → low sale price plausible, less suspicious.\n",
    "\n",
    "91: Medium-large home, good quality, basement and garage modest → sale price reasonable.\n",
    "\n",
    "2666: Large, top-quality, very old home → sale price high but consistent with features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2293485d-afb3-4fe0-bb13-5ddccca22faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare identified indices to their medians and IQRs\n",
    "\n",
    "# Flagged row indices\n",
    "flagged_indices = [1498, 2180, 2181, 1182, 2737, 2570, 1945, 1782, 91, 2666]\n",
    "\n",
    "# Key columns to check\n",
    "key_cols = ['Gr Liv Area_capped', 'Total Bsmt SF', 'Overall Qual', 'Overall Cond', 'SalePrice_capped',\n",
    "            'Year Built', 'Year Remod/Add', 'Lot Area_capped', 'BsmtFin SF 1', 'BsmtFin SF 2', \n",
    "            'Garage Cars', 'Garage Area']\n",
    "\n",
    "# Subset flagged rows\n",
    "flagged_rows = df_new.loc[flagged_indices, key_cols]\n",
    "\n",
    "# Compute median and IQR for the full dataset\n",
    "median_vals = df_new[key_cols].median()\n",
    "q1 = df_new[key_cols].quantile(0.25)\n",
    "q3 = df_new[key_cols].quantile(0.75)\n",
    "\n",
    "# Combine into a comparison DataFrame\n",
    "comparison_df = flagged_rows.copy()\n",
    "for col in key_cols:\n",
    "    comparison_df[f'{col}_median'] = median_vals[col]\n",
    "    comparison_df[f'{col}_IQR_low'] = q1[col]\n",
    "    comparison_df[f'{col}_IQR_high'] = q3[col]\n",
    "\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3649b9-0b0e-4086-8464-ac2d3ea82aeb",
   "metadata": {},
   "source": [
    "1498: Extremely large home and basement, top quality, but sale price at the lower IQR → potential data entry error.\n",
    "\n",
    "2180: Very large home, high quality, sale price below median → suspiciously low; likely outlier.\n",
    "\n",
    "2181: Large home, high quality, price slightly above 2180 but still below expected range → possible outlier.\n",
    "\n",
    "1182: Large home, high quality, very small basement, price well below median → unusual, may be an outlier.\n",
    "\n",
    "2737: Large home, moderate quality, very high condition, sale price above median → plausible, less extreme.\n",
    "\n",
    "2570: Large home, decent quality, low basement and garage space, sale price slightly above median → mildly unusual but not extreme.\n",
    "\n",
    "1945: Large home, low quality, minimal basement, price above median → inconsistent, potential outlier.\n",
    "\n",
    "1782: Smaller home, decent quality, all features within median/IQR range → reasonable, likely not an outlier.\n",
    "\n",
    "91: Medium-large home, good quality, basement and garage modest, price above median → plausible, not extreme.\n",
    "\n",
    "2666: Large, top-quality, very old home, sale price at upper end → unusual but could be valid for historic property.\n",
    "\n",
    "Most likely worth removing (extreme potential outliers):\n",
    "\n",
    "1498 – huge size, top quality, extremely low sale price.\n",
    "\n",
    "2180 – very large and high-quality, sale price unusually low.\n",
    "\n",
    "2181 – similar to 2180; large and high-quality, sale price lower than expected.\n",
    "\n",
    "1182 – very large, high quality, extremely small basement, very low price.\n",
    "\n",
    "The others are generally consistent with the dataset when compared to median/IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b2a79-8c42-4d33-8597-50e025041ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Original flagged row indices (positional in df_pruned)\n",
    "# --------------------------\n",
    "flagged_indices = [1498, 2181, 2180, 1182]\n",
    "\n",
    "# --------------------------\n",
    "# Cross-validation setup\n",
    "# --------------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Scorers\n",
    "scoring = {\n",
    "    'r2': make_scorer(r2_score),\n",
    "    'rmse': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "    'mae': make_scorer(mean_absolute_error)\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# Original model2 metrics\n",
    "# --------------------------\n",
    "# Features & target (same as model 2)\n",
    "X_cv = df_final.drop('SalePrice_capped', axis=1)\n",
    "\n",
    "# Target\n",
    "y_cv = df_final['SalePrice_capped']\n",
    "\n",
    "original_cv = cross_validate(LinearRegression(), X_cv, y_cv, cv=kf, scoring=scoring, return_train_score=True)\n",
    "original_scores = {\n",
    "    'train_r2': np.mean(original_cv['train_r2']),\n",
    "    'test_r2': np.mean(original_cv['test_r2']),\n",
    "    'train_rmse': np.mean(original_cv['train_rmse']),\n",
    "    'test_rmse': np.mean(original_cv['test_rmse']),\n",
    "    'train_mae': np.mean(original_cv['train_mae']),\n",
    "    'test_mae': np.mean(original_cv['test_mae'])\n",
    "}\n",
    "\n",
    "print(\"Original model performance (average across folds):\")\n",
    "for k, v in original_scores.items():\n",
    "    print(f\"{k}: {v:.3f}\")\n",
    "print(\"\\nRunning outlier sensitivity analysis...\\n\")\n",
    "\n",
    "# --------------------------\n",
    "# Outlier sensitivity analysis\n",
    "# --------------------------\n",
    "results = []\n",
    "\n",
    "for r in range(len(flagged_indices)+1):\n",
    "    for subset in combinations(flagged_indices, r):\n",
    "        # Drop rows by index if any\n",
    "        if subset:\n",
    "            df_subset = df_final.drop(index=list(subset))\n",
    "        else:\n",
    "            df_subset = df_final.copy()\n",
    "\n",
    "        X = df_subset.drop('SalePrice_capped', axis=1)\n",
    "        y = df_subset['SalePrice_capped']\n",
    "\n",
    "        cv = cross_validate(LinearRegression(), X, y, cv=kf, scoring=scoring, return_train_score=True)\n",
    "\n",
    "        # Record results with deltas\n",
    "        results.append({\n",
    "            'removed_indices': subset,\n",
    "            'test_r2': np.mean(cv['test_r2']),\n",
    "            'delta_test_r2': np.mean(cv['test_r2']) - original_scores['test_r2'],\n",
    "            'test_rmse': np.mean(cv['test_rmse']),\n",
    "            'delta_test_rmse': np.mean(cv['test_rmse']) - original_scores['test_rmse'],\n",
    "            'test_mae': np.mean(cv['test_mae']),\n",
    "            'delta_test_mae': np.mean(cv['test_mae']) - original_scores['test_mae']\n",
    "        })\n",
    "\n",
    "# --------------------------\n",
    "# Format results for readability\n",
    "# --------------------------\n",
    "outlier_removal_results_df = pd.DataFrame(results)\n",
    "\n",
    "# Round numbers for clarity\n",
    "for col in ['test_r2','delta_test_r2','test_rmse','delta_test_rmse','test_mae','delta_test_mae']:\n",
    "    outlier_removal_results_df[col] = outlier_removal_results_df[col].round(3)\n",
    "\n",
    "# Sort by delta_test_r2 descending (best improvements on top)\n",
    "outlier_removal_results_df = outlier_removal_results_df.sort_values('delta_test_r2', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Outlier sensitivity analysis results (compared to original model):\")\n",
    "print(outlier_removal_results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e13e44-6e2b-4b5e-a823-15624824b1eb",
   "metadata": {},
   "source": [
    "Verdict\n",
    "\n",
    "These four rows are true high-leverage / influential points.\n",
    "\n",
    "They are distorting the model, likely because their size, basement, and price are inconsistent relative to the rest of the data.\n",
    "\n",
    "Removing all four clearly improves cross-validated performance.\n",
    "\n",
    "✅ Recommendation: Remove these four rows before final modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66dcc8-4823-4a52-b30e-b0ded46bb257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 0. Create output folder\n",
    "# =============================\n",
    "output_folder = \"Outlier treatment iteration\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# =============================\n",
    "# 1. Create a new df without the outliers\n",
    "# =============================\n",
    "df_outliers_removed = df_final.drop(flagged_indices, errors=\"ignore\")\n",
    "\n",
    "# =============================\n",
    "# 2. Define predictors and response\n",
    "# =============================\n",
    "X = df_outliers_removed.drop('SalePrice_capped', axis=1)\n",
    "# Convert categories and bools to numeric\n",
    "for col in X.columns:\n",
    "    if pd.api.types.is_categorical_dtype(X[col]) or pd.api.types.is_bool_dtype(X[col]):\n",
    "        X[col] = X[col].astype(int)\n",
    "y = df_outliers_removed[\"SalePrice_capped\"]\n",
    "\n",
    "# Convert to numeric and drop any remaining NaNs\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "y = pd.to_numeric(y, errors='coerce')\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# Add constant for statsmodels\n",
    "X_sm = sm.add_constant(X)\n",
    "\n",
    "# =============================\n",
    "# 3. Fit final model\n",
    "# =============================\n",
    "model_outliers_removed = sm.OLS(y, X_sm).fit()\n",
    "\n",
    "# =============================\n",
    "# 4. Performance metrics\n",
    "# =============================\n",
    "print(\"Model Performance (with outliers removed):\")\n",
    "print(model_outliers_removed.summary())\n",
    "\n",
    "# =============================\n",
    "# 5. Residual diagnostics\n",
    "# =============================\n",
    "residuals = model_outliers_removed.resid\n",
    "fitted = model_outliers_removed.fittedvalues\n",
    "\n",
    "# QQ plot\n",
    "qq_plot_path = os.path.join(output_folder, \"QQ_Plot_Outliers_Removed.png\")\n",
    "sm.qqplot(residuals, line='45', fit=True)\n",
    "plt.title(\"QQ Plot (Outliers Removed)\")\n",
    "plt.savefig(qq_plot_path)\n",
    "plt.show()  # <-- display\n",
    "plt.close()\n",
    "\n",
    "# Residuals vs Fitted\n",
    "residuals_plot_path = os.path.join(output_folder, \"Residuals_vs_Fitted_Outliers_Removed.png\")\n",
    "plt.scatter(fitted, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Fitted (Outliers Removed)\")\n",
    "plt.savefig(residuals_plot_path)\n",
    "plt.show()  # <-- display\n",
    "plt.close()\n",
    "\n",
    "# Cook's Distance\n",
    "# Cook's distance & influence plot after outlier removal\n",
    "influence = model_outliers_removed.get_influence()\n",
    "cooks_d, _ = influence.cooks_distance\n",
    "student_resid = influence.resid_studentized_external\n",
    "leverage = influence.hat_matrix_diag\n",
    "\n",
    "# Threshold\n",
    "n, p = X_sm.shape\n",
    "threshold = 4 / n\n",
    "\n",
    "# Identify influential points\n",
    "influential_points = np.where(cooks_d > threshold)[0]\n",
    "\n",
    "# Top 10 by Cook's D\n",
    "top_10_idx = np.argsort(cooks_d)[-10:][::-1]\n",
    "top_10_df = pd.DataFrame({\n",
    "    \"index\": X.index[top_10_idx],\n",
    "    \"Cook's Distance\": cooks_d[top_10_idx],\n",
    "    \"target\": y.iloc[top_10_idx].values\n",
    "})\n",
    "\n",
    "print(f\"Cook's distance threshold: {threshold:.4f}\")\n",
    "print(f\"Number of influential points: {len(influential_points)}\")\n",
    "print(\"Top 10 influential points:\")\n",
    "print(top_10_df)\n",
    "\n",
    "# Bubble plot (scaled by Cook's D)\n",
    "plt.figure(figsize=(8,6))\n",
    "bubble_size = 500 * (cooks_d / np.nanmax(cooks_d))\n",
    "plt.scatter(leverage, student_resid, s=bubble_size, alpha=0.5)\n",
    "plt.scatter(leverage[top_10_idx], student_resid[top_10_idx],\n",
    "            color=\"red\", s=100, label=\"Top influencers\")\n",
    "plt.axhline(0, color=\"grey\", linestyle=\"--\")\n",
    "plt.xlabel(\"Leverage\")\n",
    "plt.ylabel(\"Studentized Residuals\")\n",
    "plt.title(\"Influence Plot (Cook's Distance, Outliers Removed)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_folder, \"Influence_BubblePlot_Outliers_Removed.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk test\n",
    "shapiro_test = stats.shapiro(residuals)\n",
    "print(f\"Shapiro-Wilk test: Statistic={shapiro_test.statistic:.4f}, p-value={shapiro_test.pvalue:.4f}\")\n",
    "\n",
    "# =============================\n",
    "# 6. VIF\n",
    "# =============================\n",
    "X_vif = X_sm.select_dtypes(include=[np.number]).drop(columns='const', errors='ignore')\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) \n",
    "                   for i in range(X_vif.shape[1])]\n",
    "\n",
    "print(\"\\nVariance Inflation Factors (Outliers Removed):\")\n",
    "print(vif_data.sort_values(by=\"VIF\", ascending=False).head(10))\n",
    "\n",
    "# =============================\n",
    "# 7. Cross-Validation Performance (per fold)\n",
    "# =============================\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Scoring functions\n",
    "scoring = {\n",
    "    'r2': make_scorer(r2_score),\n",
    "    'rmse': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "    'mae': make_scorer(mean_absolute_error)\n",
    "}\n",
    "\n",
    "# Run cross-validation\n",
    "cv_results = cross_validate(LinearRegression(), X, y, cv=kf, scoring=scoring, return_train_score=True)\n",
    "\n",
    "# Create DataFrame with per-fold results\n",
    "cv_df = pd.DataFrame({\n",
    "    'Train R2': cv_results['train_r2'],\n",
    "    'Test R2': cv_results['test_r2'],\n",
    "    'Train RMSE': cv_results['train_rmse'],\n",
    "    'Test RMSE': cv_results['test_rmse'],\n",
    "    'Train MAE': cv_results['train_mae'],\n",
    "    'Test MAE': cv_results['test_mae']\n",
    "})\n",
    "\n",
    "print(\"Cross-validation results (per fold):\")\n",
    "print(cv_df)\n",
    "\n",
    "# Compute average performance across folds\n",
    "avg_performance = cv_df.mean().rename(\"Average performance across folds\")\n",
    "print(\"\\nAverage performance across folds:\")\n",
    "print(avg_performance)\n",
    "\n",
    "# =============================\n",
    "# 8. Save cross-validation results and VIF to CSV\n",
    "# =============================\n",
    "cv_df.to_csv(os.path.join(output_folder, \"CV_results_Outliers_Removed.csv\"), index=False)\n",
    "vif_data.to_csv(os.path.join(output_folder, \"VIF_Outliers_Removed.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b38849-82ac-4473-9ed4-1ab7202be358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the important features\n",
    "\n",
    "# --------------------------\n",
    "# Fit Linear Regression on full data\n",
    "# --------------------------\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# --------------------------\n",
    "# Feature importance (coefficients)\n",
    "# --------------------------\n",
    "outliers_removed_feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': linreg.coef_\n",
    "})\n",
    "\n",
    "outliers_removed_model_features = outliers_removed_feature_importance_df.reindex(\n",
    "    outliers_removed_feature_importance_df['Coefficient'].abs().sort_values(ascending=False).index\n",
    ").head(10)\n",
    "\n",
    "print(outliers_removed_model_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d4a86-6781-45a7-8000-2bcdcb3fe971",
   "metadata": {},
   "source": [
    "#### 8.2 Feature Refinement and Response Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c5980-1cda-48e8-af00-6078601a6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# 0. Create output folder\n",
    "# =============================\n",
    "output_folder_yeo = \"Outlier treatment iteration Yeo\"\n",
    "os.makedirs(output_folder_yeo, exist_ok=True)\n",
    "\n",
    "# =============================\n",
    "# 1. df without outliers (reuse existing df)\n",
    "# =============================\n",
    "df_outliers_removed = df_final.drop(flagged_indices, errors=\"ignore\")\n",
    "\n",
    "# =============================\n",
    "# 2. Define predictors and response\n",
    "# =============================\n",
    "X = df_outliers_removed.drop('SalePrice_capped', axis=1)\n",
    "for col in X.columns:\n",
    "    if pd.api.types.is_categorical_dtype(X[col]) or pd.api.types.is_bool_dtype(X[col]):\n",
    "        X[col] = X[col].astype(int)\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "X = X.dropna()\n",
    "\n",
    "y = df_outliers_removed.loc[X.index, \"SalePrice_capped\"]\n",
    "\n",
    "# =============================\n",
    "# 2b. Apply Yeo-Johnson transform to response\n",
    "# =============================\n",
    "pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "y_yeo = pt.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# =============================\n",
    "# 3. Add constant for statsmodels\n",
    "# =============================\n",
    "X_sm = sm.add_constant(X)\n",
    "\n",
    "# =============================\n",
    "# 4. Fit model with transformed response\n",
    "# =============================\n",
    "model_yeo = sm.OLS(y_yeo, X_sm).fit()\n",
    "print(\"OLS Summary (Yeo-Johnson transformed response):\")\n",
    "print(model_yeo.summary())\n",
    "\n",
    "# =============================\n",
    "# 5. Residual diagnostics\n",
    "# =============================\n",
    "residuals_yeo = model_yeo.resid\n",
    "fitted_yeo = model_yeo.fittedvalues\n",
    "\n",
    "# QQ plot\n",
    "qq_plot_path = os.path.join(output_folder_yeo, \"QQ_Plot_Yeo.png\")\n",
    "sm.qqplot(residuals_yeo, line='45', fit=True)\n",
    "plt.title(\"QQ Plot (Yeo-Johnson)\")\n",
    "plt.savefig(qq_plot_path)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Residuals vs Fitted\n",
    "residuals_plot_path = os.path.join(output_folder_yeo, \"Residuals_vs_Fitted_Yeo.png\")\n",
    "plt.scatter(fitted_yeo, residuals_yeo, alpha=0.5)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Fitted (Yeo-Johnson)\")\n",
    "plt.savefig(residuals_plot_path)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Cook's distance & influence\n",
    "influence = model_yeo.get_influence()\n",
    "cooks_d, _ = influence.cooks_distance\n",
    "student_resid = influence.resid_studentized_external\n",
    "leverage = influence.hat_matrix_diag\n",
    "n, p = X_sm.shape\n",
    "threshold = 4 / n\n",
    "influential_points = np.where(cooks_d > threshold)[0]\n",
    "\n",
    "# Top 10 influencers\n",
    "top_10_idx = np.argsort(cooks_d)[-10:][::-1]\n",
    "top_10_df = pd.DataFrame({\n",
    "    \"index\": X.index[top_10_idx],\n",
    "    \"Cook's Distance\": cooks_d[top_10_idx],\n",
    "    \"target\": y.iloc[top_10_idx].values\n",
    "})\n",
    "print(f\"Cook's distance threshold: {threshold:.4f}\")\n",
    "print(f\"Number of influential points: {len(influential_points)}\")\n",
    "print(\"Top 10 influential points:\")\n",
    "print(top_10_df)\n",
    "\n",
    "# Influence bubble plot\n",
    "plt.figure(figsize=(8,6))\n",
    "bubble_size = 500 * (cooks_d / np.nanmax(cooks_d))\n",
    "plt.scatter(leverage, student_resid, s=bubble_size, alpha=0.5)\n",
    "plt.scatter(leverage[top_10_idx], student_resid[top_10_idx],\n",
    "            color=\"red\", s=100, label=\"Top influencers\")\n",
    "plt.axhline(0, color=\"grey\", linestyle=\"--\")\n",
    "plt.xlabel(\"Leverage\")\n",
    "plt.ylabel(\"Studentized Residuals\")\n",
    "plt.title(\"Influence Plot (Yeo-Johnson)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_folder_yeo, \"Influence_BubblePlot_Yeo.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk test\n",
    "shapiro_test = stats.shapiro(residuals_yeo)\n",
    "print(f\"Shapiro-Wilk test: Statistic={shapiro_test.statistic:.4f}, p-value={shapiro_test.pvalue:.4f}\")\n",
    "\n",
    "# =============================\n",
    "# 6. VIF\n",
    "# =============================\n",
    "X_vif = X_sm.select_dtypes(include=[np.number]).drop(columns='const', errors='ignore')\n",
    "vif_data = pd.DataFrame({\n",
    "    \"feature\": X_vif.columns,\n",
    "    \"VIF\": [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "})\n",
    "print(\"\\nVariance Inflation Factors (Yeo-Johnson):\")\n",
    "print(vif_data.sort_values(by=\"VIF\", ascending=False).head(10))\n",
    "\n",
    "# =============================\n",
    "# 7. Cross-validation performance\n",
    "# =============================\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = {\n",
    "    'r2': make_scorer(r2_score),\n",
    "    'rmse': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "    'mae': make_scorer(mean_absolute_error)\n",
    "}\n",
    "\n",
    "cv_results_yeo = cross_validate(LinearRegression(), X, y_yeo, cv=kf, scoring=scoring, return_train_score=True)\n",
    "cv_df_yeo = pd.DataFrame({\n",
    "    'Train R2': cv_results_yeo['train_r2'],\n",
    "    'Test R2': cv_results_yeo['test_r2'],\n",
    "    'Train RMSE': cv_results_yeo['train_rmse'],\n",
    "    'Test RMSE': cv_results_yeo['test_rmse'],\n",
    "    'Train MAE': cv_results_yeo['train_mae'],\n",
    "    'Test MAE': cv_results_yeo['test_mae']\n",
    "})\n",
    "print(\"Cross-validation results (Yeo-Johnson, per fold):\")\n",
    "print(cv_df_yeo)\n",
    "\n",
    "avg_performance_yeo = cv_df_yeo.mean().rename(\"Average performance across folds\")\n",
    "print(\"\\nAverage performance across folds (Yeo-Johnson):\")\n",
    "print(avg_performance_yeo)\n",
    "\n",
    "# Save CV and VIF\n",
    "cv_df_yeo.to_csv(os.path.join(output_folder_yeo, \"CV_results_Yeo.csv\"), index=False)\n",
    "vif_data.to_csv(os.path.join(output_folder_yeo, \"VIF_Yeo.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4f07cc-ad94-4b73-9bd1-38d3c3922486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the outliers-removed df\n",
    "df_yeo = df_outliers_removed.copy()\n",
    "df_yeo['SalePrice_capped_YJ'] = y_yeo\n",
    "\n",
    "# Check\n",
    "df_yeo[['SalePrice_capped', 'SalePrice_capped_YJ']].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21533241-e02c-43ef-bd9f-e7e412c0bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the important features\n",
    "\n",
    "# --------------------------\n",
    "# Fit Linear Regression on full data\n",
    "# --------------------------\n",
    "linreg.fit(X, y_yeo)\n",
    "\n",
    "# --------------------------\n",
    "# Feature importance (coefficients)\n",
    "# --------------------------\n",
    "yeo_feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': linreg.coef_\n",
    "})\n",
    "\n",
    "yeo_model_features = yeo_feature_importance_df.reindex(\n",
    "    yeo_feature_importance_df['Coefficient'].abs().sort_values(ascending=False).index\n",
    ").head(10)\n",
    "\n",
    "print(yeo_model_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc46dec-9169-467b-867d-e11979540c4a",
   "metadata": {},
   "source": [
    "#### 8.3 Stepwise Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b654491-709a-42d0-8032-7866097bbe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 0. Output folder\n",
    "# =============================\n",
    "output_folder = \"Stepwise_Iteration\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# =============================\n",
    "# 1. Prepare data\n",
    "# =============================\n",
    "X = df_yeo.drop(['SalePrice_capped_YJ','SalePrice_capped'], axis=1).copy()\n",
    "y = df_yeo['SalePrice_capped_YJ'].copy()\n",
    "\n",
    "# Encode categorical/bool features as int\n",
    "for col in X.columns:\n",
    "    if pd.api.types.is_categorical_dtype(X[col]) or pd.api.types.is_bool_dtype(X[col]):\n",
    "        X[col] = X[col].astype(int)\n",
    "\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "y = pd.to_numeric(y, errors='coerce')\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# Add constant for statsmodels\n",
    "X_sm = sm.add_constant(X)\n",
    "\n",
    "# =============================\n",
    "# 2. Stepwise Feature Selection\n",
    "# =============================\n",
    "def stepwise_selection(X, y, criterion='aic', verbose=True):\n",
    "    \"\"\"Perform forward-backward stepwise selection using AIC or BIC.\"\"\"\n",
    "    initial_features = []\n",
    "    best_features = initial_features.copy()\n",
    "    remaining_features = list(X.columns)\n",
    "    current_score, best_new_score = np.inf, np.inf\n",
    "\n",
    "    while remaining_features:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in remaining_features:\n",
    "            features_to_test = best_features + [candidate]\n",
    "            model = sm.OLS(y, sm.add_constant(X[features_to_test])).fit()\n",
    "            score = model.aic if criterion.lower() == 'aic' else model.bic\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_candidate = scores_with_candidates[0]\n",
    "\n",
    "        if current_score > best_new_score:\n",
    "            remaining_features.remove(best_candidate)\n",
    "            best_features.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "            if verbose:\n",
    "                print(f\"Add {best_candidate}: {criterion.upper()}={best_new_score:.4f}\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return best_features\n",
    "\n",
    "# Run stepwise for AIC and BIC\n",
    "selected_aic = stepwise_selection(X, y, criterion='aic')\n",
    "selected_bic = stepwise_selection(X, y, criterion='bic')\n",
    "\n",
    "# =============================\n",
    "# 3. Fit final models on full dataset (for diagnostics)\n",
    "# =============================\n",
    "model_aic = sm.OLS(y, sm.add_constant(X[selected_aic])).fit()\n",
    "model_bic = sm.OLS(y, sm.add_constant(X[selected_bic])).fit()\n",
    "\n",
    "# =============================\n",
    "# 4. Residual Diagnostics & Plots\n",
    "# =============================\n",
    "def plot_diagnostics(model, name):\n",
    "    residuals = model.resid\n",
    "    fitted = model.fittedvalues\n",
    "\n",
    "    # Residuals vs Fitted\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(fitted, residuals, alpha=0.5)\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.xlabel(\"Fitted values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(f\"Residuals vs Fitted ({name})\")\n",
    "    plt.savefig(os.path.join(output_folder, f\"Residuals_vs_Fitted_{name}.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # QQ plot\n",
    "    sm.qqplot(residuals, line='45', fit=True)\n",
    "    plt.title(f\"QQ Plot ({name})\")\n",
    "    plt.savefig(os.path.join(output_folder, f\"QQ_Plot_{name}.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Shapiro-Wilk\n",
    "    shapiro_test = stats.shapiro(residuals)\n",
    "    print(f\"{name} - Shapiro-Wilk: Stat={shapiro_test.statistic:.4f}, p={shapiro_test.pvalue:.4e}\")\n",
    "\n",
    "    # Cook's Distance & Influence\n",
    "    influence = model.get_influence()\n",
    "    cooks_d, _ = influence.cooks_distance\n",
    "    leverage = influence.hat_matrix_diag\n",
    "    student_resid = influence.resid_studentized_external\n",
    "    threshold = 4 / n\n",
    "    influential_points = np.where(cooks_d > threshold)[0]\n",
    "\n",
    "\n",
    "    # Top 10 influential points\n",
    "    top_10_idx = np.argsort(cooks_d)[-10:][::-1]\n",
    "    top_10_df = pd.DataFrame({\n",
    "        \"index\": X.index[top_10_idx],\n",
    "        \"Cook's Distance\": cooks_d[top_10_idx],\n",
    "        \"target\": y.iloc[top_10_idx].values\n",
    "    })\n",
    "    print(f\"Cook's distance threshold: {threshold:.4f}\")\n",
    "    print(f\"Number of influential points: {len(influential_points)}\")\n",
    "    print(f\"Top 10 influential points ({name}):\\n\", top_10_df)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    bubble_size = 500 * (cooks_d / np.nanmax(cooks_d))\n",
    "    plt.scatter(leverage, student_resid, s=bubble_size, alpha=0.5)\n",
    "    plt.scatter(leverage[top_10_idx], student_resid[top_10_idx],\n",
    "            color=\"red\", s=100, label=\"Top influencers\")\n",
    "    plt.axhline(0, color='grey', linestyle='--')\n",
    "    plt.xlabel(\"Leverage\")\n",
    "    plt.ylabel(\"Studentized Residuals\")\n",
    "    plt.title(f\"Influence Bubble Plot ({name})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, f\"Influence_BubblePlot_{name}.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "   \n",
    "\n",
    "    # VIF\n",
    "    X_vif = sm.add_constant(X[selected_aic]).select_dtypes(include=[np.number]).drop(columns='const', errors='ignore')\n",
    "    vif_data = pd.DataFrame({\n",
    "        \"feature\": X_vif.columns,\n",
    "        \"VIF\": [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "    })\n",
    "    print(f\"Variance Inflation Factors ({name}):\\n\", vif_data.sort_values(by=\"VIF\", ascending=False).head(10))\n",
    "    vif_data.to_csv(os.path.join(output_folder, f\"VIF_{name}.csv\"), index=False)\n",
    "\n",
    "plot_diagnostics(model_aic, \"Stepwise_AIC\")\n",
    "plot_diagnostics(model_bic, \"Stepwise_BIC\")\n",
    "\n",
    "# =============================\n",
    "# 5. Cross-Validation Performance (use previous folds!)\n",
    "# =============================\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # Use your existing CV splits if stored\n",
    "\n",
    "scoring = {\n",
    "    'r2': make_scorer(r2_score),\n",
    "    'rmse': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "    'mae': make_scorer(mean_absolute_error)\n",
    "}\n",
    "\n",
    "cv_results_aic = cross_validate(LinearRegression(), X[selected_aic], y, cv=kf, scoring=scoring, return_train_score=True)\n",
    "cv_results_bic = cross_validate(LinearRegression(), X[selected_bic], y, cv=kf, scoring=scoring, return_train_score=True)\n",
    "\n",
    "def summarize_cv(cv_results, name):\n",
    "    cv_df = pd.DataFrame({\n",
    "        'Train R2': cv_results['train_r2'],\n",
    "        'Test R2': cv_results['test_r2'],\n",
    "        'Train RMSE': cv_results['train_rmse'],\n",
    "        'Test RMSE': cv_results['test_rmse'],\n",
    "        'Train MAE': cv_results['train_mae'],\n",
    "        'Test MAE': cv_results['test_mae']\n",
    "    })\n",
    "    print(f\"Cross-validation results ({name}):\\n\", cv_df)\n",
    "    avg_performance = cv_df.mean().rename(\"Average performance across folds\")\n",
    "    print(f\"\\nAverage performance ({name}):\\n\", avg_performance)\n",
    "    cv_df.to_csv(os.path.join(output_folder, f\"CV_results_{name}.csv\"), index=False)\n",
    "\n",
    "summarize_cv(cv_results_aic, \"Stepwise_AIC\")\n",
    "summarize_cv(cv_results_bic, \"Stepwise_BIC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f68aae4-a0c5-4acd-aa2b-e67dce1e0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# New dataframe for BIC model\n",
    "# =============================\n",
    "\n",
    "# Features and target restricted to BIC\n",
    "X_bic = X[selected_bic].copy()\n",
    "y_bic = y.copy()\n",
    "\n",
    "# Combine into a single dataframe\n",
    "BIC_df = X_bic.copy()\n",
    "BIC_df['SalePrice_capped_YJ'] = y_bic\n",
    "\n",
    "print(f\"BIC_df shape: {BIC_df.shape}\")\n",
    "BIC_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938fc26c-c89f-4e0b-8f0e-5c59327d2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the important features\n",
    "\n",
    "# --------------------------\n",
    "# Fit Linear Regression on full data\n",
    "# --------------------------\n",
    "linreg.fit(X_bic, y_bic)\n",
    "\n",
    "# --------------------------\n",
    "# Feature importance (coefficients)\n",
    "# --------------------------\n",
    "BIC_importance_df = pd.DataFrame({\n",
    "    'Feature': X_bic.columns,\n",
    "    'Coefficient': linreg.coef_\n",
    "})\n",
    "\n",
    "bic_model_features = BIC_importance_df.reindex(\n",
    "    BIC_importance_df['Coefficient'].abs().sort_values(ascending=False).index\n",
    ").head(10)\n",
    "\n",
    "print(bic_model_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a634c25-cf9b-41d2-a6f6-e7028ea12a26",
   "metadata": {},
   "source": [
    "#### 8.4 Regularization (Ridge / Lasso / Elastic Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85436d3-0bc2-45cf-a71d-a302e0fc8164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 0. Output folder\n",
    "# =============================\n",
    "output_folder = \"Regularization_Models\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# =============================\n",
    "# 1. Prepare data\n",
    "# =============================\n",
    "X = BIC_df.drop('SalePrice_capped_YJ', axis=1).copy()\n",
    "y = BIC_df['SalePrice_capped_YJ'].copy()\n",
    "\n",
    "# Standardize numeric features for regularization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "# =============================\n",
    "# 2. Cross-validation setup\n",
    "# =============================\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = {\n",
    "    'r2': make_scorer(r2_score),\n",
    "    'rmse': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "    'mae': make_scorer(mean_absolute_error)\n",
    "}\n",
    "\n",
    "# =============================\n",
    "# 3. Hyperparameter grids\n",
    "# =============================\n",
    "ridge_params = {'alpha': np.logspace(-3, 3, 10)}\n",
    "lasso_params = {'alpha': np.logspace(-3, 1, 10)}\n",
    "elastic_params = {'alpha': np.logspace(-3, 1, 10), 'l1_ratio': [0.2, 0.5, 0.8]}\n",
    "\n",
    "# =============================\n",
    "# 4. Fit models with GridSearchCV\n",
    "# =============================\n",
    "def fit_model(model, params, X, y, cv):\n",
    "    gs = GridSearchCV(model, params, cv=cv, scoring='r2', n_jobs=-1)\n",
    "    gs.fit(X, y)\n",
    "    best_model = gs.best_estimator_\n",
    "    print(f\"{model.__class__.__name__} Best Params: {gs.best_params_}\")\n",
    "    return best_model\n",
    "\n",
    "ridge_best = fit_model(Ridge(), ridge_params, X_scaled, y, kf)\n",
    "lasso_best = fit_model(Lasso(max_iter=10000), lasso_params, X_scaled, y, kf)\n",
    "elastic_best = fit_model(ElasticNet(max_iter=10000), elastic_params, X_scaled, y, kf)\n",
    "\n",
    "# =============================\n",
    "# 5. Residual Diagnostics\n",
    "# =============================\n",
    "def plot_residuals(model, X, y, name):\n",
    "    y_pred = model.predict(X)\n",
    "    residuals = y - y_pred\n",
    "    \n",
    "    # Residuals vs Fitted\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.xlabel(\"Fitted values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(f\"Residuals vs Fitted ({name})\")\n",
    "    plt.savefig(os.path.join(output_folder, f\"Residuals_vs_Fitted_{name}.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # QQ Plot\n",
    "    sm.qqplot(residuals, line='45', fit=True)\n",
    "    plt.title(f\"QQ Plot ({name})\")\n",
    "    plt.savefig(os.path.join(output_folder, f\"QQ_{name}.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Shapiro-Wilk\n",
    "    shapiro_test = stats.shapiro(residuals)\n",
    "    print(f\"{name} - Shapiro-Wilk: Stat={shapiro_test.statistic:.4f}, p={shapiro_test.pvalue:.4e}\")\n",
    "    \n",
    "    # Cook's distance using OLS on residuals\n",
    "    X_sm = sm.add_constant(X)\n",
    "    ols_model = sm.OLS(y, X_sm).fit()\n",
    "    influence = ols_model.get_influence()\n",
    "    cooks_d, _ = influence.cooks_distance\n",
    "    leverage = influence.hat_matrix_diag\n",
    "    student_resid = influence.resid_studentized_external\n",
    "    threshold = 4 / n\n",
    "    influential_points = np.where(cooks_d > threshold)[0]\n",
    "\n",
    "\n",
    "    # Top 10 influential points\n",
    "    top_10_idx = np.argsort(cooks_d)[-10:][::-1]\n",
    "    top_10_df = pd.DataFrame({\n",
    "        \"index\": X.index[top_10_idx],\n",
    "        \"Cook's Distance\": cooks_d[top_10_idx],\n",
    "        \"target\": y.iloc[top_10_idx].values\n",
    "    })\n",
    "    print(f\"Cook's distance threshold: {threshold:.4f}\")\n",
    "    print(f\"Number of influential points: {len(influential_points)}\")\n",
    "    print(f\"Top 10 influential points ({name}):\\n\", top_10_df)\n",
    "    \n",
    "    # Influence plot (bubble)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    bubble_size = 500 * (cooks_d / np.nanmax(cooks_d))\n",
    "    plt.scatter(leverage, student_resid, s=bubble_size, alpha=0.5)\n",
    "    plt.scatter(leverage[top_10_idx], student_resid[top_10_idx], color=\"red\", s=100, label=\"Top influencers\")\n",
    "    plt.axhline(0, color='grey', linestyle='--')\n",
    "    plt.xlabel(\"Leverage\")\n",
    "    plt.ylabel(\"Studentized Residuals\")\n",
    "    plt.title(f\"Influence Bubble Plot ({name})\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_folder, f\"Influence_Bubble_{name}.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # VIF\n",
    "    X_vif = sm.add_constant(X).select_dtypes(include=[np.number]).drop(columns='const', errors='ignore')\n",
    "    vif_data = pd.DataFrame({\n",
    "        \"feature\": X_vif.columns,\n",
    "        \"VIF\": [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "    })\n",
    "    print(f\"Variance Inflation Factors ({name}):\\n\", vif_data.sort_values(by=\"VIF\", ascending=False).head(10))\n",
    "    vif_data.to_csv(os.path.join(output_folder, f\"VIF_{name}.csv\"), index=False)\n",
    "\n",
    "# Apply to all models\n",
    "plot_residuals(ridge_best, X_scaled, y, \"Ridge\")\n",
    "plot_residuals(lasso_best, X_scaled, y, \"Lasso\")\n",
    "plot_residuals(elastic_best, X_scaled, y, \"ElasticNet\")\n",
    "\n",
    "# =============================\n",
    "# 6. Cross-Validation Performance\n",
    "# =============================\n",
    "def cross_val_metrics(model, X, y, cv, name):\n",
    "    cv_results = cross_validate(model, X, y, cv=cv, scoring=scoring, return_train_score=True)\n",
    "    cv_df = pd.DataFrame({\n",
    "        'Train R2': cv_results['train_r2'],\n",
    "        'Test R2': cv_results['test_r2'],\n",
    "        'Train RMSE': cv_results['train_rmse'],\n",
    "        'Test RMSE': cv_results['test_rmse'],\n",
    "        'Train MAE': cv_results['train_mae'],\n",
    "        'Test MAE': cv_results['test_mae']\n",
    "    })\n",
    "    print(f\"Cross-validation results ({name}):\\n\", cv_df)\n",
    "    avg_perf = cv_df.mean().rename(\"Average performance across folds\")\n",
    "    print(f\"\\nAverage performance ({name}):\\n\", avg_perf)\n",
    "    cv_df.to_csv(os.path.join(output_folder, f\"CV_results_{name}.csv\"), index=False)\n",
    "\n",
    "cross_val_metrics(ridge_best, X_scaled, y, kf, \"Ridge\")\n",
    "cross_val_metrics(lasso_best, X_scaled, y, kf, \"Lasso\")\n",
    "cross_val_metrics(elastic_best, X_scaled, y, kf, \"ElasticNet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d339437-e2b1-4a35-afe6-453350230f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the important features\n",
    "ridge_importance_df = pd.DataFrame({\n",
    "    'Feature': X_scaled.columns,\n",
    "    'Coefficient': ridge_best.coef_\n",
    "})\n",
    "\n",
    "ridge_model_features = ridge_importance_df.reindex(\n",
    "    ridge_importance_df['Coefficient'].abs().sort_values(ascending=False).index\n",
    ").head(10)\n",
    "\n",
    "print(ridge_model_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83397c8-892b-48b0-9fc0-6162aa8ecb69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
